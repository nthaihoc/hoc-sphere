{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to HocSphere","text":"<p>Xin ch\u00e0o! t\u00f4i l\u00e0 H\u1ecdc, k\u1ef9 s\u01b0 v\u00e0 nh\u00e0 nghi\u00ean c\u1ee9u AI t\u1ea1i Institute of Applied Science Technology - Vi\u1ec7n Khoa h\u1ecdc &amp; C\u00f4ng ngh\u1ec7 \u1ee8ng d\u1ee5ng, thu\u1ed9c University of Information Technology and Communication - Tr\u01b0\u1eddng \u0110\u1ea1i h\u1ecdc C\u00f4ng ngh\u1ec7 Th\u00f4ng tin &amp; Truy\u1ec1n th\u00f4ng. T\u00f4i t\u1eadp trung nghi\u00ean c\u1ee9u v\u00e0 ph\u00e1t tri\u1ec3n c\u00e1c gi\u1ea3i ph\u00e1p AI to\u00e0n di\u1ec7n, \u1ee9ng d\u1ee5ng Machine Learning, Computer Vision v\u00e0 Large Language Models v\u00e0o l\u0129nh v\u1ef1c ch\u0103m s\u00f3c s\u1ee9c kh\u1ecfe v\u00e0 gi\u00e1o d\u1ee5c. B\u00ean c\u1ea1nh \u0111\u00f3, t\u00f4i c\u0169ng \u0111am m\u00ea kh\u00e1m ph\u00e1 nh\u1eefng k\u1ef9 thu\u1eadt AI ti\u00ean ti\u1ebfn trong nhi\u1ec1u l\u0129nh v\u1ef1c da d\u1ea1ng nh\u01b0 \u1ee9ng d\u1ee5ng 3D, ph\u01b0\u01a1ng ti\u1ec7n t\u1ef1 h\u00e0nh v\u00e0 x\u1eed l\u00fd d\u1eef li\u1ec7u l\u1edbn. B\u1ea1n c\u00f3 th\u1ec1 t\u00ecm hi\u1ec3u th\u00eam v\u1ec1 t\u00f4i t\u1ea1i website.</p> <p>HocSphere l\u00e0 n\u1ec1n t\u1ea3ng t\u00f4i x\u00e2y d\u1ef1ng \u0111\u1ec3 l\u01b0u tr\u1eef v\u00e0 chia s\u1ebb tri th\u1ee9c. \u0110\u00e2y l\u00e0 kh\u00f4ng gian trung t\u00e2m n\u01a1i t\u00f4i t\u1ed5ng h\u1ee3p t\u00e0i li\u1ec7u nghi\u00ean c\u1ee9u, ghi ch\u00e9p l\u1ea1i c\u00e1c quan s\u00e1t c\u00e1 nh\u00e2n, v\u00e0 kh\u00e1m ph\u00e1 nhi\u1ec1u n\u1ed9i dung xoay quanh th\u1ebf gi\u1edbi AI kh\u00f4ng ng\u1eebng thay \u0111\u1ed5i. T\u00f4i hi v\u1ecdng r\u1eb1ng n\u01a1i \u0111\u00e2y s\u1ebd tr\u1edf th\u00e0nh m\u1ed9t t\u00e0i nguy\u00ean h\u1eefu \u00edch - n\u01a1i b\u1ea1n c\u00f3 th\u1ec3 t\u00ecm th\u1ea5y \u0111\u00fang \u0111i\u1ec1u m\u00ecnh c\u1ea7n.</p> \\[\\textbf{~ Nguy\u1ec5n Th\u00e1i H\u1ecdc ~}\\]"},{"location":"course-notes/csdl/","title":"Deep Learning Course","text":""},{"location":"course-notes/csdl/#i-description","title":"I. Description","text":"<p>This course provides a comprehensive journey into Deep Learning, covering everything from fundamental neural network concepts to cutting-edge architecture like CNNs, LSTMs, Transformers and GAN. You will gain hands-on experience in building, training, and deploying AI models using Tensorflow/Pytorch.</p>"},{"location":"course-notes/csdl/#ii-what-youll-learn","title":"II. What You'll learn","text":"<ul> <li>Neural networks, backpropagation and optimization techniques</li> <li>Convolutional Neural Networks for image processing</li> <li>Recurrent Neural Networks, Long Short Term Memory for sequential data</li> <li>Transformers &amp; Attention mechanisms</li> <li>Generative Adversarial Networks for AI-generated content</li> <li>Hyperparameter tuning and hyperparameter methods</li> </ul>"},{"location":"course-notes/csdl/#iii-prerequisites","title":"III. Prerequisites","text":"<p>To take this course, you should have a basic understanding of Python programming, along with fundamental concepts in linear algebra, probability, statistics and machine learning. Prior experience with Tensorflow/Pytorch is helpful but not required.</p>"},{"location":"course-notes/csdl/#iv-lecture-schedule","title":"IV. Lecture Schedule","text":"Lecture Title Description Status Resources 01 Introduction in Deep Learning (-) Key concepts in Deep Learning  (-) Real-world applications of Deep Learning  (-) Overview of neural networks and their structure 02 Neural Networks and Backpropagation (-) Neural network structure  (-) Activation functions  (-) Feedforward process  (-) Backpropagation &amp; Gradient Descent 03 Loss functions and Optimizers (-) Cross-Entropy, Huber Loss, Mean Squared Error  (-) Stochastic Gradient Descent, Momentum, Nesterov Accelerated Gradient  (-) AdaGrad, RMSProp, Adam, AdamW, LARS 04 Regularization and Batch Normalization (-) Overfitting vs. Underfitting  L1 (Lasso) &amp; L2 (Ridge) regularization, Dropout, Early stopping  (-) Batch Normalization 05 Advanced Neural Networks (-) Convolutional Neural Networks  (-) Popular CNN architecture  (-) Residual connections 06 Recurrent Neural Networks &amp; Long Short-Term Memory (-) RNN process sequential data  (-) LSTM in handling long-range dependencies  (-) Gated Recurrent Unit (GRU) 07 Transformers &amp; Attention Mechanism (-) Key concepts  (-) Self-Attention &amp; Multi-Head Attention  (-) Transformer architecture 08 Generative Models (-) Introduction to generative models  (-) GAN architecture  (-) Popular GAN variants 09 Hyperparameter Tuning (-) Choosing the best hyperparameters  (-) Hyperparameter optimization methods"},{"location":"course-notes/csdl/#v-acknowledgements","title":"V. Acknowledgements","text":"<p>This course is inspired by the collective knowledge and contributions of the deep learning community. I would like to extend my gratitude to researchers, educators and institutions whose work has shaped modern AI, including Stanford CS230 - Deep Learning (Andrew Ng), and key research papers on neural networks, transformers and generative models.</p> <p></p>"},{"location":"course-notes/csllm/","title":"Large Language Models (LLMs)","text":""},{"location":"course-notes/csllm/#i-thong-tin-khoa-hoc","title":"I. Th\u00f4ng tin kh\u00f3a h\u1ecdc","text":"<p>M\u00f4 h\u00ecnh ng\u00f4n ng\u1eef l\u1edbn (Large Language Models - LLMs) \u0111ang v\u00e0 s\u1ebd ti\u1ebfp t\u1ee5c l\u00e0 ch\u1ee7 \u0111\u1ec1 n\u00f3ng b\u1ecfng v\u00e0 nh\u1eadn \u0111\u01b0\u1ee3c nhi\u1ec1u s\u1ef1 quan t\u00e2m c\u1ee7a c\u00e1c nh\u00e0 nghi\u00ean c\u1ee9u v\u00e0 nh\u1eefng c\u00f4ng ty, t\u1eadp \u0111o\u00e0n l\u1edbn tr\u00ean th\u1ebf gi\u1edbi v\u1ec1 l\u0129nh v\u1ef1c tr\u00ed tu\u1ec7 nh\u00e2n t\u1ea1o (Artificial Intelligence - AI). Sau nh\u1eefng s\u1ef1 th\u00e0nh c\u00f4ng c\u1ee7a OpenAI - khi cho ra m\u1eaft m\u00f4 h\u00ecnh ChatGPT v\u00e0o cu\u1ed1i n\u0103m 2022 v\u00e0 m\u1edbi \u0111\u00e2y nh\u1ea5t l\u00e0 s\u1ef1 ph\u00e1t tri\u1ec3n c\u1ee7a DeepSeek, c\u00e1c m\u00f4 h\u00ecnh LLMs g\u1ea7n nh\u01b0 \u0111\u00e3 ch\u1ee9ng t\u1ecf \u0111\u01b0\u1ee3c ti\u1ec1m n\u0103ng to l\u1edbn trong vi\u1ec7c gi\u1ea3i quy\u1ebft c\u00e1c b\u00e0i to\u00e1n ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean, t\u1eeb tr\u1ea3 l\u1eddi c\u00e2u h\u1ecfi, t\u1ea1o n\u1ed9i dung \u0111\u1ebfn h\u1ed7 tr\u1ee3 c\u00e1c \u1ee9ng d\u1ee5ng ph\u1ee9c t\u1ea1p kh\u00e1c.</p> <p>Kh\u00f3a h\u1ecdc n\u00e0y \u0111\u01b0\u1ee3c thi\u1ebft k\u1ebf d\u1ef1a tr\u00ean vi\u1ec7c ghi ch\u00fa l\u1ea1i nh\u1eefng ki\u1ebfn th\u1ee9c quan tr\u1ecdng v\u00e0 n\u1ed5i b\u1eadt theo ch\u1ee7 \u0111\u1ec1 c\u1ee7a LLMs gi\u00fap b\u1ea1n n\u1eafm v\u1eefng \u0111\u01b0\u1ee3c nh\u1eefng ki\u1ebfn th\u1ee9c c\u01a1 b\u1ea3n c\u0169ng nh\u01b0 c\u00e1c k\u1ef9 thu\u1eadt ti\u00ean ti\u1ebfn trong vi\u1ec7c tri\u1ec3n khai v\u00e0 x\u00e2y d\u1ef1ng c\u00e1c m\u00f4 h\u00ecnh ng\u00f4n ng\u1eef l\u1edbn t\u1eeb \u0111\u1ea7u, t\u01b0\u01a1ng t\u1ef1 nh\u01b0 c\u00e1c m\u00f4 h\u00ecnh n\u1ed5i b\u1eadt nh\u01b0 GPT c\u1ee7a OpenAI.</p>"},{"location":"course-notes/csllm/#ii-ban-se-hoc-uoc-nhung-gi","title":"II. B\u1ea1n s\u1ebd h\u1ecdc \u0111\u01b0\u1ee3c nh\u1eefng g\u00ec?","text":"<p>Kh\u00f4ng ch\u1ec9 d\u1eebng l\u1ea1i \u1edf vi\u1ec7c cung c\u1ea5p l\u00fd thuy\u1ebft, m\u00e0 kh\u00f3a h\u1ecdc c\u00f2n t\u1eadp trung v\u00e0o th\u1ef1c h\u00e0nh, gi\u00fap b\u1ea1n c\u00f3 th\u1ec3 t\u1ef1 x\u00e2y d\u1ef1ng v\u00e0 t\u1ed1i \u01b0u m\u1ed9t m\u00f4 h\u00ecnh GPT-like t\u1eeb \u0111\u1ea7u. Kh\u00f3a h\u1ecdc s\u1ebd h\u1ed7 tr\u1ee3 b\u1ea1n:</p> <ul> <li> <p>Hi\u1ec3u v\u00e0 n\u1eafm b\u1eaft \u0111\u01b0\u1ee3c nh\u1eefng kh\u00e1i ni\u1ec7m quan tr\u1ecdng c\u1ee7a LLMs: Ki\u1ebfn tr\u00fac Transformer, Attention Mechanism, Tokenization, v.v.</p> </li> <li> <p>X\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh LLMs t\u1eeb \u0111\u1ea7u: D\u00f9ng Python, PyTorch \u0111\u1ec3 thi\u1ebft k\u1ec3 m\u00f4 h\u00ecnh t\u01b0\u01a1ng t\u1ef1 nh\u01b0 ChatGPT.</p> </li> <li> <p>Hu\u1ea5n luy\u1ec7n v\u00e0 t\u1ed1i \u01b0u h\u00f3a: Pre-trainining, Fine-tuning, c\u1ea3i thi\u1ec7n hi\u1ec7u su\u1ea5t v\u1edbi c\u00e1c k\u1ef9 thu\u1eadt t\u1ed1i \u01b0u h\u00f3a.</p> </li> </ul>"},{"location":"course-notes/csllm/#iii-yeu-cau","title":"III. Y\u00eau c\u1ea7u","text":"<p>\u0110\u1ec3 c\u00f3 th\u1ec3 d\u1ec5 d\u00e0ng theo d\u00f5i v\u00e0 n\u1eafm b\u1eaft \u0111\u01b0\u1ee3c c\u00e1c n\u1ed9i dung trong kh\u00f3a h\u1ecdc n\u00e0y, b\u1ea1n c\u1ea7n c\u00f3 n\u1ec1n t\u1ea3ng v\u1eefng ch\u1eafc \u1edf m\u1ed9t s\u1ed1 kh\u00eda c\u1ea1nh nh\u01b0:</p> <ul> <li> <p>Ki\u1ebfn th\u1ee9c l\u1eadp tr\u00ecnh: Th\u00e0nh th\u1ea1o ng\u00f4n ng\u1eef Python (bi\u1ebft v\u1ec1 Numpy, Pandas, Tensorflow ho\u1eb7c PyTorch l\u00e0 l\u1ee3i th\u1ebf).</p> </li> <li> <p>Hi\u1ec3u v\u1ec1 Machine Learning: N\u1eafm v\u1eefng \u0111\u01b0\u1ee3c c\u00e1c thu\u1eadt to\u00e1n h\u1ecdc c\u00f3 gi\u00e1m s\u00e1t (supervised learning) v\u00e0 m\u1ea1ng t\u00edch ch\u1eadp (neural network) c\u01a1 b\u1ea3n.</p> </li> <li> <p>To\u00e1n h\u1ecdc c\u01a1 b\u1ea3n: Hi\u1ec3u v\u1ec1 \u0111\u1ea1i s\u1ed1 tuy\u1ebfn t\u00ednh, x\u00e1c su\u1ea5t, th\u1ed1ng k\u00ea v\u00e0 gi\u1ea3i t\u00edch.</p> </li> <li> <p>Kinh nghi\u1ec7m l\u00e0m vi\u1ec7c v\u1edbi d\u1eef li\u1ec7u: Bi\u1ebft c\u00e1ch thu th\u1eadp v\u00e0 ti\u1ec1n x\u1eed l\u00fd, l\u1eb1m s\u1ea1ch d\u1eef li\u1ec7u v\u0103n b\u1ea3n.</p> </li> </ul>"},{"location":"course-notes/csllm/#iv-lich-trinh-bai-giang","title":"IV. L\u1ecbch tr\u00ecnh b\u00e0i gi\u1ea3ng","text":"STT Ch\u1ee7 \u0111\u1ec1 M\u00f4 t\u1ea3 Tr\u1ea1ng th\u00e1i T\u00e0i li\u1ec7u 01 T\u1ed5ng quan v\u1ec1 Large Language Model (LLMs) (--) Kh\u00e1i ni\u1ec7m v\u00e0 \u01b0ng d\u1ee5ng c\u1ee7a LLMs.  (--) Quy tr\u00ecnh x\u00e2y d\u1ef1ng v\u00e0 s\u1eed d\u1ee5ng c\u00e1c m\u00f4 h\u00ecnh LLMs.  (--) Gi\u1edbi thi\u1ec7u v\u1ec1 ki\u1ebfn tr\u00fac Transformer.  (--) T\u00ecm hi\u1ec3u chi ti\u1ebft ki\u1ebfn tr\u00fac GPT N/A 02 Nghi\u00ean c\u1ee9u v\u00e0 l\u00e0m vi\u1ec7c v\u1edbi d\u1eef li\u1ec7u v\u0103n b\u1ea3n 03 Tri\u1ec3n khai c\u01a1 ch\u1ebf Attention trong LLMs: T\u1eeb Self-Attention \u0111\u1ebfn Multi-Head Attention (--) T\u01b0 duy c\u00e1ch th\u1ee9c ho\u1ea1t \u0111\u1ed9ng c\u1ee7a self-attention.  (--) C\u00e1c b\u01b0\u1edbc tri\u1ec3n khai chi ti\u1ebft self-attention trong Python.  (--) \u1ee8ng d\u1ee5ng causal masking \u0111\u1ec3 ki\u1ec3m so\u00e1t th\u00f4ng tin trong c\u00e1c m\u00f4 h\u00ecnh sinh chu\u1ed7i.  (--) K\u1ef9 thu\u1eadt m\u1edf r\u1ed9ng t\u1eeb attention m\u1ed9t \u0111\u1ea7u sang attention nhi\u1ec1u \u0111\u1ea7u. Pending"},{"location":"course-notes/csllm/llm03/","title":"Tri\u1ec3n khai c\u01a1 ch\u1ebf Attention trong LLMs: T\u1eeb Self-Attention \u0111\u1ebfn Multi-Head Attention","text":"<p>Trong b\u00e0i gi\u1ea3ng tr\u01b0\u1edbc, ch\u00fang ta \u0111\u00e3 c\u00f9ng th\u1ea3o lu\u1eadn v\u00e0 c\u00e1c b\u1ea1n \u0111\u00e3 bi\u1ebft c\u00e1ch chu\u1ea9n b\u1ecb v\u0103n b\u1ea3n \u0111\u1ea7u v\u00e0o \u0111\u1ec3 hu\u1ea5n luy\u1ec7n c\u00e1c m\u00f4 h\u00ecnh ng\u00f4n ng\u1eef l\u1edbn b\u1eb1ng c\u00e1ch chia nh\u1ecf v\u0103n b\u1ea3n th\u00e0nh c\u00e1c token v\u00e0 subword, sau \u0111\u00f3 th\u1ef1c hi\u1ec7n m\u00e3 h\u00f3a ch\u00fang th\u00e0nh c\u00e1c bi\u1ec3u di\u1ec5n vector - hay c\u00f2n g\u1ecdi l\u00e0 embedding. Ngay b\u00e2y gi\u1edd, ch\u00fang ta s\u1ebd t\u00ecm hi\u1ec3u m\u1ed9t ph\u1ea7n c\u1ed1t l\u00f5i, tr\u00e1i tim c\u1ee7a h\u1ea7u h\u1ebft c\u00e1c ki\u1ebfn tr\u00fac LLM - \u0111\u00f3 l\u00e0 c\u01a1 ch\u1ebf ch\u00fa \u00fd (attention mechanisms), nh\u01b0 \u0111\u01b0\u1ee3c minh h\u1ecda trong h\u00ecnh 3.1. Ch\u00fang ta s\u1ebd ch\u1ee7 y\u1ebfu t\u00ecm hi\u1ec3u c\u00e1c c\u01a1 ch\u1ebf attention m\u1ed9t c\u00e1ch ri\u00eang bi\u1ec7t v\u00e0 t\u1eadp trung v\u00e0o c\u00e1ch th\u1ee9c ho\u1ea1t \u0111\u1ed9ng. Ti\u1ebfp theo, ti\u1ebfn h\u00e0nh l\u1eadp tr\u00ecnh c\u00e1c ph\u1ea7n c\u00f2n l\u1ea1i c\u1ee7a LLM xung quanh c\u01a1 ch\u1ebf self-attention (t\u1ef1 ch\u00fa \u00fd) \u0111\u1ec3 quan s\u00e1t c\u00e1ch ch\u00fang ho\u1ea1t \u0111\u1ed9ng v\u00e0 x\u00e2y d\u1ef1ng m\u1ed9t m\u00f4 h\u00ecnh c\u00f3 kh\u1ea3 n\u0103ng sinh v\u0103n b\u1ea3n.</p> H\u00ecnh 3.1:Nh\u1eefng giai \u0111o\u1ea1n quan tr\u1ecdng \u0111\u1ec3 x\u00e2y d\u1ef1ng m\u1ed9t m\u00f4 h\u00ecnh LLM. <p>C\u00f3 r\u1ea5t nhi\u1ec1u bi\u1ebfn th\u1ec3 kh\u00e1c nhau c\u1ee7a c\u01a1 ch\u1ebf attention, trong \u0111\u00f3 \u0111i\u1ec3n h\u00ecnh l\u00e0 b\u1ed1n (nh\u01b0 \u0111\u01b0\u1ee3c minh h\u1ecda trong H\u00ecnh 3.2). Nh\u1eefng bi\u1ebfn th\u1ec3 n\u00e0y \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng k\u1ebf ti\u1ebfp nhau, v\u00e0 m\u1ee5c ti\u00eau \u0111\u1ea1t \u0111\u01b0\u1ee3c \u0111\u00f3 l\u00e0 hi\u1ec7n th\u1ef1c h\u00f3a c\u01a1 ch\u1ebf multi-head attention (ch\u00fa \u00fd \u0111a \u0111\u1ea7u) m\u1ed9t c\u00e1ch hi\u1ec7u qu\u1ea3 v\u00e0 d\u1ec5 hi\u1ec3u, \u0111\u00e2y l\u00e0 c\u01a1 ch\u1ebf n\u1ec1n t\u1ea3ng \u0111\u1ec3 t\u00edch h\u1ee3p ch\u00fang v\u00e0o ki\u1ebfn tr\u00fac LLM s\u1ebd \u0111\u01b0\u1ee3c l\u1eadp tr\u00ecnh trong ch\u01b0\u01a1ng ti\u1ebfp theo.</p> H\u00ecnh 3.2:Nh\u1eefng bi\u1ebfn th\u1ec3 \u0111i\u1ec3n h\u00ecnh c\u1ee7a c\u01a1 ch\u1ebf attention s\u1ebd \u0111\u01b0\u1ee3c th\u1ea3o lu\u1eadn trong ch\u01b0\u01a1ng n\u00e0y. <p>3.1 V\u1ea5n \u0111\u1ec1 khi x\u1eed l\u00fd c\u00e1c chu\u1ed7i d\u00e0i c\u1ee7a c\u00e1c m\u00f4 h\u00ecnh x\u1eed l\u00fd ng\u00f4n ng\u1eef truy\u1ec1n th\u1ed1ng.</p> <p>Tr\u01b0\u1edbc khi \u0111i l\u1ea7n l\u01b0\u1ee3t v\u00e0o c\u01a1 ch\u1ebf self-attention, \u0111\u1ea7u ti\u00ean h\u00e3y c\u00f9ng xem x\u00e9t v\u1ea5n \u0111\u1ec1 m\u00e0 c\u00e1c ki\u1ebfn tr\u00fac tr\u01b0\u1edbc th\u1eddi k\u1ef3 LLM g\u1eb7p ph\u1ea3i (nh\u1eefng m\u00f4 h\u00ecnh kh\u00f4ng s\u1eed d\u1ee5ng attention).</p> <p>Gi\u1ea3 s\u1eed ch\u00fang ta mu\u1ed1n x\u00e2y d\u1ef1ng m\u1ed9t m\u00f4 h\u00ecnh d\u1ecbch ng\u00f4n ng\u1eef (language translation) nh\u1eb1m chuy\u1ec3n \u0111\u1ed5i v\u0103n b\u1ea3n t\u1eeb m\u1ed9t ng\u00f4n ng\u1eef n\u00e0y sang m\u1ed9t ng\u00f4n ng\u1eef kh\u00e1c. Ngh\u0129 m\u1ed9t c\u00e1ch n\u00f4m na, ta ch\u1ec9 c\u1ea7n l\u00e0m cho m\u00f4 h\u00ecnh \u0111\u01a1n gi\u1ea3n nh\u1ea5t, l\u00e0 d\u1ecbch t\u1eebng t\u1eeb m\u1ed9t (minh h\u1ecda trong H\u00ecnh 3.3). Tuy nhi\u00ean \u0111i\u1ec1u n\u00e0y l\u00e0 kh\u00f4ng th\u1ec3, b\u1edfi v\u00ec c\u1ea5u tr\u00fac ng\u1eef ph\u00e1p gi\u1eefa c\u00e1c ng\u00f4n ng\u1eef g\u1ed1c v\u00e0 ng\u00f4n ng\u1eef \u0111\u00edch th\u01b0\u1eddng r\u1ea5t kh\u00e1c nhau.</p> H\u00ecnh 3.3:M\u00f4 ph\u1ecfng m\u00f4 h\u00ecnh d\u1ecbch ng\u00f4n ng\u1eef d\u1ecbch t\u1eeb ti\u1ebfng vi\u1ec7t sang ti\u1ebfng anh. <p>\u0110\u1ec3 gi\u1ea3i quy\u1ebft v\u1ea5n \u0111\u1ec1 n\u00e0y, c\u00e1c nh\u00e0 nghi\u00ean c\u1ee9u \u0111\u00e3 s\u1eed d\u1ee5ng m\u00f4 h\u00ecnh transformer - m\u1ed9t m\u1ea1ng n\u01a1-ron s\u00e2u bao g\u1ed3m hai m\u00f4-\u0111un con: encoder (b\u1ed9 m\u00e3 h\u00f3a) v\u00e0 decoder (b\u1ed9 gi\u1ea3i m\u00e3). Nhi\u1ec7m v\u1ee5 c\u1ee7a encoder l\u00e0 \u0111\u1ecdc v\u00e0 x\u1eed l\u00fd to\u00e0n b\u1ed9 v\u0103n b\u1ea3n \u0111\u1ea7u v\u00e0o, sau \u0111\u00f3 decoder s\u1ebd t\u1ea1o ra v\u0103n b\u1ea3n \u0111\u00e3 \u0111\u01b0\u1ee3c d\u1ecbch.</p> <p>Tr\u01b0\u1edbc khi c\u00e1c m\u00f4 h\u00ecnh transformer ra \u0111\u1eddi, c\u00e1c m\u1ea1ng n\u01a1-ron h\u1ed3i ti\u1ebfp (RNN - Recurrent Neural Networks) l\u00e0 ki\u1ebfn tr\u00fac encoder-decoder ph\u1ed5 bi\u1ebfn nh\u1ea5t trong c\u00e1c m\u00f4 h\u00ecnh d\u1ecbch ng\u00f4n ng\u1eef. RNN l\u00e0 m\u1ed9t lo\u1ea1i m\u1ea1ng n\u01a1-ron trong \u0111\u00f3 \u0111\u1ea7u ra c\u1ee7a b\u01b0\u1edbc tr\u01b0\u1edbc s\u1ebd l\u00e0 \u0111\u1ea7u v\u00e0o cho b\u01b0\u1edbc hi\u1ec7n t\u1ea1i, \u0111i\u1ec1u n\u00e0y l\u00e0m ch\u00fang ph\u00f9 h\u1ee3p \u0111\u1ec3 x\u1eed l\u00fd d\u1eef li\u1ec7u tu\u1ea7n t\u1ef1 nh\u01b0 v\u0103n b\u1ea3n. </p> <p>Trong ki\u1ebfn tr\u00fac encoder-decoder c\u1ee7a RNN, v\u0103n b\u1ea3n \u0111\u1ea7u v\u00e0o \u0111\u01b0\u1ee3c \u0111\u01b0a v\u00e0o encoder, n\u01a1i ch\u00fang \u0111\u01b0\u1ee3c x\u1eed l\u00fd tu\u1ea7n t\u1ef1 t\u1eebng b\u01b0\u1edbc. Encoder s\u1ebd c\u1eadp nh\u1eadt tr\u1ea1ng th\u00e1i \u1ea9n (hidden state) c\u1ee7a d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o t\u1ea1i m\u1ed7i b\u01b0\u1edbc, m\u1ee5c ti\u00eau ch\u00ednh l\u00e0 n\u0103m b\u1eaft to\u00e0n b\u1ed9 \u00fd ngh\u0129a c\u1ee7a c\u00e2u \u0111\u1ea7u v\u00e0o trong tr\u1ea1ng th\u00e1i \u1ea9n cu\u1ed1i c\u00f9ng nh\u01b0 \u0111\u01b0\u1ee3c minh h\u1ecda trong H\u00ecnh 3.4.</p> <p>Decoder sau \u0111\u00f3 s\u1ebd s\u1eed d\u1ee5ng c\u00e1c tr\u1ea1ng th\u00e1i \u1ea9n cu\u1ed1i c\u00f9ng m\u00e0 encoder t\u1ea1o ra, \u0111\u1ec3 b\u1eaft \u0111\u1ea7u t\u1ea1o ra c\u00e2u d\u1ecbch t\u1eebng t\u1eeb m\u1ed9t. Ch\u00fang c\u0169ng c\u1eadp nh\u1eadt lu\u00f4n c\u00e1c tr\u1ea1ng th\u00e1i \u1ea9n \u1edf m\u1ed7i b\u01b0\u1edbc - tr\u1ea1ng th\u00e1i n\u00e0y c\u00f3 th\u1ec3 coi l\u00e0 nh\u1eefng th\u00f4ng tin ng\u1eef c\u1ea3nh quan tr\u1ecdng, c\u1ea7n thi\u1ebft \u0111\u1ec3 \u0111\u01b0a ra d\u1ef1 \u0111o\u00e1n t\u1eeb k\u1ebf ti\u1ebfp.</p> H\u00ecnh 3.4:B\u1ed9 gi\u1ea3i m\u00e3 encoder-decoder c\u1ee7a RNN khi x\u1eed l\u00fd d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o v\u00e0 sinh ra c\u00e2u d\u1ecbch. <p>\u00dd t\u01b0\u1edfng c\u1ed1t l\u00f5i c\u1ee7a ki\u1ebfn tr\u00fac transformer \u1edf \u0111\u00e2y l\u00e0: ph\u1ea7n encoder s\u1ebd \u0111\u1ea3m nhi\u1ec7m x\u1eed l\u00fd to\u00e0n b\u1ed9 v\u0103n b\u1ea3n \u0111\u1ea7u v\u00e0o th\u00e0nh m\u1ed9t tr\u1ea1ng th\u00e1i \u1ea9n - hay c\u00f2n g\u1ecdi l\u00e0 b\u1ed9 nh\u1edb t\u1ea1m (memory cell). Decoder sau \u0111\u00f3 s\u1ebd s\u1eed d\u1ee5ng tr\u1ea1ng th\u00e1i \u1ea9n n\u00e0y \u0111\u1ec3 sinh \u0111\u1ea7u ra. B\u1ea1n c\u00f3 th\u1ec3 xem tr\u1ea1ng th\u00e1i \u1ea9n nh\u01b0 m\u1ed9t vector embedding, kh\u00e1i ni\u1ec7m m\u00e0 \u0111\u00e3 \u0111\u01b0\u1ee3c nh\u1eafc t\u1edbi trong ch\u01b0\u01a1ng 2.</p> <p>\u0110i\u1ec3m h\u1ea1n ch\u1ebf l\u1edbn c\u1ee7a m\u00f4 h\u00ecnh encoder-decoder RNN l\u00e0: trong giai \u0111o\u1ea1n gi\u1ea3i m\u00e3, ch\u00fang kh\u00f4ng th\u1ec3 truy c\u1eadp tr\u1ef1c ti\u1ebfp v\u00e0o c\u00e1c tr\u1ea1ng th\u00e1i \u1ea9n tr\u01b0\u1edbc \u0111\u00f3 t\u1eeb encoder. Do v\u1eady, ch\u00fang ch\u1ec9 d\u1ef1a v\u00e0o tr\u1ea1ng th\u00e1i \u1ea9n hi\u1ec7n t\u1ea1i, tr\u1ea1ng th\u00e1i n\u00e0y c\u00f3 th\u1ec3 coi l\u00e0 g\u00f3i to\u00e0n b\u1ed9 th\u00f4ng tin ng\u1eef c\u1ea3nh. \u0110i\u1ec1u n\u00e0y c\u00f3 th\u1ec3 d\u1eabn \u0111\u1ebfn vi\u1ec7c m\u1ea5t ng\u1eef c\u1ea3nh, \u0111\u1eb7c bi\u1ec7t l\u00e0 v\u1edbi nh\u1eefng c\u00e2u ph\u1ee9c t\u1ea1p c\u00f3 c\u00e1c m\u1ed1i li\u00ean h\u1ec7 d\u00e0i gi\u1eefa c\u00e1c th\u00e0nh ph\u1ea7n trong c\u00e2u. Nh\u01b0ng s\u1ef1 xu\u1ea5t hi\u1ec7n c\u1ee7a nh\u1eefng h\u1ea1n ch\u1ebf n\u00e0y l\u1ea1i l\u00e0 \u0111\u1ed9ng l\u1ef1c \u0111\u1ec3 c\u00e1c c\u01a1 ch\u1ebf attention ra \u0111\u1eddi.</p> <p>3.2 N\u1eafm b\u1eaft m\u1ed1i quan h\u1ec7 ph\u1ee5 thu\u1ed9c d\u1eef li\u1ec7u b\u1eb1ng c\u01a1 ch\u1ebf attention.</p> <p>M\u1eb7c d\u00f9 RNN ho\u1ea1t \u0111\u1ed9ng kh\u00e1 t\u1ed1t khi d\u1ecbch c\u00e1c c\u00e2u ng\u1eafn, nh\u01b0ng ch\u00fang kh\u00f4ng ho\u1ea1t \u0111\u1ed9ng hi\u1ec7u qu\u1ea3 v\u1edbi c\u00e1c v\u0103n b\u1ea3n d\u00e0i, do kh\u00f4ng th\u1ec3 truy c\u1eadp tr\u1ef1c ti\u1ebfp v\u00e0o c\u00e1c t\u1eeb tr\u01b0\u1edbc \u0111\u00f3 trong \u0111\u1ea7u v\u00e0o. M\u1ed9t h\u1ea1n ch\u1ebf l\u1edbn c\u1ee7a c\u00e1ch ti\u1ebfp c\u1eadn n\u00e0y l\u00e0: RNN ph\u1ea3i ghi nh\u1edb to\u00e0n b\u1ed9 c\u00e2u \u0111\u00e3 \u0111\u01b0\u1ee3c m\u00e3 h\u00f3a trong m\u1ed9t tr\u1ea1ng th\u00e1i \u1ea9n duy nh\u1ea5t tr\u01b0\u1edbc khi chuy\u1ec3n \u0111\u1ebfn decoder (H\u00ecnh 3.4).</p> <p>V\u00e0o n\u0103m 2014, c\u00e1c nh\u00e0 nghi\u00ean c\u1ee9u \u0111\u00e3 ph\u00e1t tri\u1ec3n c\u01a1 ch\u1ebf attention Bahdanau cho RNN - \u0111\u01b0\u1ee3c \u0111\u1eb7t theo t\u00ean t\u00e1c gi\u1ea3 \u0111\u1ea7u ti\u00ean c\u1ee7a b\u00e0i b\u00e1o. C\u01a1 ch\u1ebf n\u00e0y ch\u1ec9nh s\u1eeda ki\u1ebfn tr\u00fac encoder-decoder, cho ph\u00e9p decoder c\u00f3 th\u1ec3 ch\u1ecdn l\u1ecdc truy c\u1eadp v\u00e0o c\u00e1c ph\u1ea7n t\u1eed kh\u00e1c nhau c\u1ee7a chu\u1ed7i \u0111\u1ea7u v\u00e0o t\u1ea1i m\u1ed7i b\u01b0\u1edbc gi\u1ea3i m\u00e3, minh ho\u1ea1 trong H\u00ecnh 3.5.</p>"},{"location":"course-notes/csml/","title":"Machine Learning Course","text":""},{"location":"course-notes/csml/#i-description","title":"I. Description","text":"<p>This course provides knowledge on artificial intelligence, with a particular focus on machine learning. Learners will gain a solid understanding of core machine learning concepts, including key algorithms for supervised and unsupervised learning, as well as widely-used techniques in the field. The course also guides learners on how to comprehensively set up, implement, and evaluate models, helping them understand the full process of building a machine learning system from start to finish.</p>"},{"location":"course-notes/csml/#ii-what-youll-learn","title":"II. What You'll learn","text":"<ul> <li> <p>Grasp the core concepts and mechanics of foundational machine learning algorithms.</p> </li> <li> <p>Develop complete machine learning models using TensorFlow, scikit-learn, and Keras, covering  algorithms for regression (like Linear and Lasso Regression) and classification (such as SVM, Naive Bayes, Decision Trees).</p> </li> <li> <p>Master techniques for fine-tuning, evaluating, and optimizing machine learning models to enhance their performance and accuracy.</p> </li> </ul>"},{"location":"course-notes/csml/#iii-prerequisites","title":"III. Prerequisites","text":"<ul> <li> <p>Have a strong foundation in linear algebra, probability, statistics, and basic calculus, enabling you to understand the mathematical concepts behind machine learning.</p> </li> <li> <p>Be comfortable with Python programming, with practical experience in writing and debugging code for machine learning tasks.</p> </li> <li> <p>Have hands-on experience working with popular machine learning libraries, including TensorFlow, Keras, and scikit-learn, to implement and experiment with various algorithms.</p> </li> </ul>"},{"location":"course-notes/csml/#iv-lecture-schedule","title":"IV. Lecture Schedule","text":"Lecture Title Description Status Resources 01 Introduction to AI and Machine Learning (-) Overview of AI and Machine Learning  (-) Types of Machine Learning  (-) Real-world applications of Machine Learning 02 Regression Analysis (-) Linear Regression  (-) Multiple Linear Regression  (-) Polynomial Regression 03 Classification Algorithms (-) Logistic Regression  (-) Performance metrics: Precision, Recall, F1-score, AUC-ROC 04 Model Evaluation &amp; Regularization (-) Train-Test split &amp; Cross validation  (-) Bias-Variance trade-off  (-) Regularization techniques: L1, L2 05 Naive Bayes Classifier (-) Bayes theorem &amp; Naive Bayes assumptions  (-) Gaussian, multinomial and Bernoulli Naive Bayes 06 Support Vector Machine (-) SVM intuition &amp; mathematics  (-) Kernel trick: Linear &amp; Non-linear SVM 07 Decision Tree (-) Decision Tree structure  (-) Entropy, gini impurity 08 Ensemble Methods (-) Random Forest algorithm  (-) Ensemble learning techniques: Bagging vs. Booting vs. Stacking 09 Dimensionality Reduction Techniques (-) Principal Component Analysis  (-) Singular Value Decomposition 10 Unsupervised Learning &amp; Clustering (-) K-Means Clustering  (-) Gaussian Mixture Models 11 Feature Engineering &amp; Data Preprocessing (-) Handling missing data  (-) Feature scaling &amp; encoding  (-) Feature selection &amp; extraction 12 Hyperparameter Tuning &amp; Model Optimization (-) Grid search, Random Search, Bayesian optimization  (-) Learning rate schedulers"},{"location":"course-notes/csml/#v-acknowledgments","title":"V. Acknowledgments","text":"<p>I would like to express my heartfelt gratitude to two authors: Andrew Ng, the creator of the CS229 Machine Learning course, and Aur\u00e9lien G\u00e9ron, the author of the book Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow. Their work has been a tremendous source of inspiration and motivation for me to create these lecture notes. </p> <p></p>"},{"location":"knowledge-base/","title":"Knowlegde Base","text":""},{"location":"knowledge-base/data-structure-and-algorithm/","title":"Data Structures and Algorithms","text":""},{"location":"knowledge-base/data-structure-and-algorithm/#i-overview","title":"I. Overview","text":""},{"location":"knowledge-base/data-structure-and-algorithm/#ii-knowledge-base","title":"II. Knowledge Base","text":""},{"location":"knowledge-base/data-structure-and-algorithm/#iii-resources","title":"III. Resources","text":""},{"location":"knowledge-base/linear-algebra/","title":"Linear Algebra","text":""},{"location":"knowledge-base/linear-algebra/#i-overview","title":"i. Overview","text":"<p>This collection of notes provides a comprehensive overview of Linear Algebra, covering fundamental concepts essential for understanding machine learning, data science, and mathematics. Topics include:</p> <ul> <li>Vectors and Vector Spaces: Basic building blocks and their properties.</li> <li>Matrices: Operations, transformations, and solving systems of linear equations.</li> <li>Eigenvalues and Eigenvectors: Key concepts for dimensionality reduction and matrix decompositions.</li> <li>Linear Transformations: Mappings between vector spaces.</li> </ul> <p>Linear Algebra is crucial for many AI algorithms and data analysis tasks. These notes summarize the core principles and applications, offering both theoretical and practical insights.</p>"},{"location":"knowledge-base/linear-algebra/#ii-knowledge-base","title":"ii. Knowledge Base","text":"No. Title Description Status Resources 01 Introduction to Algebra (Part 1) Vector and Matrices, Eigenvalues and Eigenvectors, Principle Component Analysis, Singular Value Decomposition Progress [Notes] [Code]"},{"location":"knowledge-base/linear-algebra/#iii-resources","title":"iii. Resources","text":""},{"location":"research-logs/","title":"Research Notes","text":"<p><code>Research notes (ghi ch\u00fa nghi\u00ean c\u1ee9u), t\u1ed5ng h\u1ee3p l\u1ea1i nh\u1eefng ghi ch\u00fa, ki\u1ebfn th\u1ee9c, v\u00e0 t\u00e0i li\u1ec7u \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng t\u1eeb c\u00e1c d\u1ef1 \u00e1n th\u1ef1c t\u1ebf v\u00e0 qu\u00e1 tr\u00ecnh t\u1ef1 nghi\u00ean c\u1ee9u c\u00e1 nh\u00e2n, nh\u1eb1m ph\u1ee5c v\u1ee5 vi\u1ec7c h\u1ecdc t\u1eadp, ph\u00e2n t\u00edch v\u00e0 ph\u00e1t tri\u1ec3n gi\u1ea3i ph\u00e1p.</code></p>"},{"location":"research-logs/#research-topics","title":"Research Topics","text":"<ul> <li> <p> Cervical Cancer Cytology - T\u1ebf b\u00e0o h\u1ecdc Ung th\u01b0 c\u1ed5 t\u1eed cung</p> <p></p> <p> Xem chi ti\u1ebft</p> </li> <li> <p> T\u1ebf b\u00e0o h\u1ecdc Ung th\u01b0 c\u1ed5 t\u1eed cung (Cervical Cancer Cytology)</p> <p></p> <p> Xem chi ti\u1ebft</p> </li> </ul> <p></p>"},{"location":"research-logs/3d-object-recognition-and-reconstruction/","title":"3d object recognition and reconstruction","text":"<p>Quote</p> <p>Weekly task progress summary of the labeling tool project, including: research notes, compiled documents, and source code.</p>"},{"location":"research-logs/cervical-cancer-cytology/","title":"Cervical Cancer Cytology","text":"<p>\u0110\u00e2y l\u00e0 d\u1ef1 \u00e1n \u0111\u01b0\u1ee3c nghi\u00ean c\u1ee9u v\u00e0 tri\u1ec3n khai b\u1edfi Vi\u1ec7n KH &amp; CNUD v\u1edbi s\u1ef1 h\u1ee3p t\u00e1c c\u1ee7a B\u1ec7nh vi\u1ec7n A - Th\u00e1i Nguy\u00ean nh\u1eb1m t\u1ea1o ra m\u1ed9t h\u1ec7 th\u1ed1ng AI c\u00f3 kh\u1ea3 n\u0103ng h\u1ed7 tr\u1ee3 c\u00e1c b\u00e1c s\u0129 chuy\u00ean khoa t\u1ebf b\u00e0o h\u1ecdc trong vi\u1ec7c s\u00e0ng l\u1ecdc, d\u1ef1 b\u00e1o v\u00e0 t\u01b0 v\u1ea5n li\u00ean quan \u0111\u1ebfn b\u1ec7nh Ung th\u01b0 c\u1ed5 t\u1eed cung d\u1ef1a tr\u00ean h\u00ecnh \u1ea3nh t\u1ebf b\u00e0o h\u1ecdc.</p>"},{"location":"research-logs/cervical-cancer-cytology/#research-logbook-so-tay-nghien-cuu","title":"Research Logbook - S\u1ed5 tay nghi\u00ean c\u1ee9u","text":"Deadline Name Status Description Materials 16/09 -&gt; 16/11, 2024 Nghi\u00ean c\u1ee9u Ensemble Learning cho ph\u00e2n lo\u1ea1i Ung th\u01b0 c\u1ed5 t\u1eed cung t\u1ebf b\u00e0o h\u1ecdc (--) Nghi\u00ean c\u1ee9u k\u1ef9 thu\u1eadt Ensemble Leanring v\u00e0 c\u00e1c ph\u01b0\u01a1ng ph\u00e1p c\u1ee7a ch\u00fang.  (--) L\u00ean ph\u01b0\u01a1ng \u00e1n th\u1ef1c nghi\u1ec7m cho b\u1ed9 d\u1eef li\u1ec7u th\u1ef1c t\u1ebf.  (--) Tri\u1ec3n khai \u0111\u00e1nh gi\u00e1, v\u00e0 c\u00e0i \u0111\u1eb7t hu\u1ea5n luy\u1ec7n nh\u1eb1m so s\u00e1nh hi\u1ec7u su\u1ea5t gi\u1eefa m\u00f4 h\u00ecnh \u0111\u01a1n l\u1ebb v\u00e0 m\u00f4 h\u00ecnh k\u1ebft h\u1ee3p. [Paper] [Slides] [Code] 06/01/2025 -&gt; Current Nghi\u00ean c\u1ee9u n\u1ec1n t\u1ea3ng &amp; \u0111\u1ecbnh h\u01b0\u1edbng m\u00f4 h\u00ecnh s\u1eed d\u1ee5ng Self-Supervised Learning (Ph\u1ea7n 01) (--) Nghi\u00ean c\u1ee9u t\u1ed5ng quan v\u1ec1 Self-Supervised Learning (SSL).  (--) T\u00ecm hi\u1ec3u v\u1ec1 m\u1ed9t s\u1ed1 m\u00f4 h\u00ecnh SSL ph\u1ed5 bi\u1ebfn: BYOL, MoCo, SimCLR.  (--) Ph\u00e2n t\u00edch chi ti\u1ebft ki\u1ebfn tr\u00fac c\u1ee7a SimCLR: backbone, loss function, augmentation &amp; projection head, v.v. [Paper] [Slides] Thi\u1ebft k\u1ebf &amp; tri\u1ec3n khai m\u00f4 h\u00ecnh SimCLR (Ph\u1ea7n 02) (--) C\u00e0i \u0111\u1eb7t ki\u1ebfn tr\u00fac SimCLR, \u0111\u00e1nh gi\u00e1 t\u00ednh ph\u00f9 h\u1ee3p v\u00e0 hu\u1ea5n luy\u1ec7n cho b\u00e0i to\u00e1n ph\u00e2n lo\u1ea1i t\u1ebf b\u00e0o h\u1ecdc.  (--) L\u00ean ph\u01b0\u01a1ng \u00e1n th\u1ef1c nghi\u1ec7m nhanh, sau \u0111\u00f3 \u0111\u00e1nh gi\u00e1 k\u1ebft qu\u1ea3 s\u01a1 b\u1ed9 v\u00e0 l\u00ean k\u1ebf ho\u1ea1ch \u0111i\u1ec1u ch\u1ec9nh.  (--)T\u1ed1i \u01b0u h\u00f3a v\u00e0 m\u1edf r\u1ed9ng tinh ch\u1ec9nh, gia t\u0103ng hi\u1ec7u su\u1ea5t cho m\u00f4 h\u00ecnh. [Repo] [Notes] [Code]"},{"location":"research-logs/cervical-cancer-cytology/#quick-links-lien-ket-nhanh","title":"Quick Links - Li\u00ean k\u1ebft nhanh","text":"<p>To\u00e0n b\u1ed9 materials (t\u00e0i li\u1ec7u tham kh\u1ea3o) v\u00e0  resources (m\u00e3 ngu\u1ed3n) c\u1ee7a nh\u1eadt k\u00fd nghi\u00ean c\u1ee9u c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c truy c\u1eadp nhanh t\u1ea1i \u0111\u01b0\u1eddng d\u1eabn sau.</p> <p>Resources  </p> <p>Materials </p>"},{"location":"research-logs/cervical-cancer-cytology/#datasets-bo-du-lieu","title":"Datasets - B\u1ed9 d\u1eef li\u1ec7u","text":"<p>B\u1ed9 d\u1eef li\u1ec7u v\u1ec1 ung th\u01b0 c\u1ed5 t\u1eed cung n\u00e0y \u0111\u01b0\u1ee3c thu th\u1eadp v\u00e0 g\u00e1n nh\u00e3n th\u1ee7 c\u00f4ng b\u1edfi c\u00e1c b\u00e1c s\u0129 chuy\u00ean khoa t\u1ea1i B\u1ec7nh vi\u1ec7n A, Th\u00e1i Nguy\u00ean. D\u1eef li\u1ec7u hi\u1ec7n \u0111\u00e3 \u0111\u01b0\u1ee3c c\u00f4ng khai, ph\u1ee5c v\u1ee5 cho m\u1ee5c \u0111\u00edch tham kh\u1ea3o v\u00e0 nghi\u00ean c\u1ee9u.</p> <p>Atlat Datasets </p> <p> Account: BOCSDL@ai4med.com  Password: BenhvienAThaiNguyen </p> <p></p>"},{"location":"research-logs/ccc-notes/note01/","title":"Self-Supervised Learning Model: A Simple Framework for Contrastive Learning of Visual Representations","text":""},{"location":"research-logs/ccc-notes/note01/#outlines","title":"Outlines","text":"<ul> <li>I. Introduction</li> <li>II. Methods</li> <li>III. Implementation plan</li> </ul>"},{"location":"research-logs/ccc-notes/note01/#i-introduction","title":"I. Introduction","text":"<p>(+) Key findings of SimCLR</p> <ul> <li>Composition of data augmentations plays a critical role in defining effective predictive tasks.</li> <li>Nonlinear transformation between the representations and the contrastive loss substantially improves the quality of the learned representations.</li> <li>Benefits from larger batch sizes and more training steps compared to supervised learning.</li> </ul> <p>(+) Main notes</p> <ul> <li>Most mainstream approaches fall into one of two group: generative and discriminative.</li> <li>Discriminative approaches use objective functions similar to those used for supervised learning, but train networks to perform pretext tasks where both the inputs and labels are created from an unlabeled dataset.<ul> <li>Based on contrastive learning in the latent space.</li> <li>More better than heuristics to design pretext tasks.</li> </ul> </li> </ul>"},{"location":"research-logs/ccc-notes/note01/#ii-methods","title":"II. Methods","text":"<p>(+) Contrastive Learning Framework</p> <ul> <li>Data augmentation: Stochastic transforms any given data example randomly resulting in two correlated views of the same example (\\(x_i\\) and \\(x_j\\)).</li> <li>Base encoder: \\(f(.)\\) that extracts representations vectors from augmented data examples. Where \\(h_i \\in \\mathbb{R}^d\\) is output of the average pooling layer.</li> <li>Projection head: \\(g(.)\\) that maps representations to the space latent where contrastive loss is applied.</li> <li>Contrastive loss function: Given set \\(x_k\\) including a positive pair of examples \\(x_i\\) and \\(x_j\\). Aims to identify \\(x_j \\in x_{k}\\) with \\(k \\neq i\\) for a given \\(x_i\\).</li> </ul> \\[  L_(i,j) = -log \\frac{exp(sim(z_i, z_j)/ \\tau)}{\\sum^{2N}_{k=1} \\mathbb{1}_{k \\neq i} \\exp(sim(z_i, z_k)/ \\tau)}  \\] <ul> <li>Where \\(sim(z_i, z_j)\\) is cosine similar between \\(z_i\\) and \\(z_j\\) vector, \\(\\tau\\) is temperature parameter.   </li> </ul> <p>(+) SimCLR pseudo code</p> <p>Important steps in SimCLR </p> <ol> <li>Data augmentation from an example data.</li> <li>Feature extraction is performed on all examples using a pre-trained backbone, and then they are mapped to the latent space by the projection head.</li> <li>In the latent space, compute cosine similarity for all examples (including positive and negative pair).</li> <li>Compute the contrastive loss function based on cosine similarity of all examples computed in step 3.</li> <li>Finally, update the networks \\(f(.)\\) and \\(g(.)\\) to minimize \\(\\mathcal{L} \\text{(loss function)}\\). Retain the encoder network \\(f(.)\\) and discard \\(g(.)\\). </li> </ol> <p>(+) Discussion</p> <p>Data</p> <ul> <li>Data augmentation defines predictive tasks.</li> <li>Composition of data augmentation operations is crucial for learning good representations.</li> <li>Contrastive learning needs stronger data augmentation than supervised learning.</li> </ul> <p>Architectures</p> <ul> <li>Unsupervised learning benefits more from bigger models than its supervised counterpart.</li> </ul> <ul> <li>Nonlinear projection head improves the representation quality of the layer before it.</li> </ul> <p>Loss Functions</p> <ul> <li>Logistic Loss $$ \\mathcal{L(y, \\hat{y})} = -y \\text{ } log(\\hat{y}) - (1 - y) \\text{ } log(1 - \\hat{y}) $$</li> </ul> <p>Disadvantages : Cannot effectively leverage hard negatives.</p> <ul> <li>Margin Loss $$ \\mathcal{L}_\\text{margin} = \\max (0, m + d_p - d_n) $$</li> </ul> <p>Disadvanteges : Choose the appropriate margin \\(m\\) value. Focuses semi-hard negatives but identify semi-hard negatives sample is very hard. </p> <ul> <li>NT-Xent Loss</li> </ul> \\[  L_(i,j) = -log \\frac{exp(sim(z_i, z_j)/ \\tau)}{\\sum^{2N}_{k=1} \\mathbb{1}_{k \\neq i} \\exp(sim(z_i, z_k)/ \\tau)}  \\] <p>Advantages : Leverages both positive and negative samples. Use cosine similarity. Automatically focuses on hard negatives using the temperature parameter \\(\\tau\\). </p>"},{"location":"research-logs/ccc-notes/note01/#iii-implementation-plan","title":"III. Implementation plan","text":"<p>(!!!) Some of the training strategies and approaches for the cervical cancer dataset</p> <p>Stage 1: Choose the model backbone</p> <p>Using some pre-trained models on the ImageNet dataset:</p> <ul> <li>Directly use pre-trained models as the backbone and then perform fine-tuning on a small amount of labeled data.</li> <li>Leverage pre-trained models and then continue training the model on an unlabeled dataset.</li> <li>Train the model from scratch with dataset consisting solely of cellular images.</li> </ul> <p>Stage 2: Evaluation the model backbone</p> <p>After the model has been trained on unlabeled data, where representations are learned in latent space using NT-Xent loss, followed by evaluation with some linear layers.</p> <ul> <li>Retain the weights of the model trained on unlabeled data, then add some linear layers and continue training the model on a small amount of labeled data.</li> <li>Evaluate the performance of the model on validation and test data.</li> </ul> <p>Stage 3: Fine-tune the model for the downstream task</p> <p>Perform fine-tuning on the model that has the best performance after evaluation in stage 2 on labeled dataset. </p> <ul> <li>Fine-tune the model sequentially with different proportions of labeled data (10%, 20%, 30%, 40%, 50%) and evaluate its performance.</li> </ul> <p></p>"},{"location":"research-logs/ccc-notes/note02/","title":"Implementation SimCLR Model","text":""},{"location":"research-logs/ccc-notes/note02/#outlines","title":"Outlines","text":"<ul> <li>I. End-to-End Model Training Process</li> <li>II. NT-Xent Loss</li> <li>III. LARS Optimizer</li> </ul>"},{"location":"research-logs/ccc-notes/note02/#i-end-to-end-model-training-process","title":"I. End-to-End Model Training Process","text":"<pre><code>flowchart LR\n\n    subgraph task01[Pretext Task]\n\n        A(((x))) --&gt; |AG'| x_1([x_1])\n        A(((x))) --&gt; |AG'| x_2([x_2])\n\n        x_1 --&gt; en01[base encoder]\n        x_2 --&gt; en01\n\n        en01 --&gt; |feature map| h_1{{ h_1 }}\n        en01 --&gt; |feature map| h_2{{ h_2 }}\n\n\n        h_1 --&gt; |MLP'| z_1[[z_1]]\n        h_2 --&gt; |MLP'| z_2[[z_2]]\n\n        z_1 --&gt; |LN'| linear{linear layers}\n        z_2 --&gt; |LN'| linear\n\n    end\n\n    subgraph task02[Downstream Task]\n    inputs(((x))) ---&gt; |inputs| en02[base encoder]\n\n    en02 --&gt; finetune{{ fine_tune }}\n    labels(((y))) ---&gt; |labels| finetune\n\n    finetune --&gt; |evaluation| eval01{Precision/Recall/F1}\n    end\n\n    en01 --&gt; en02</code></pre> <p>(+) Model training process consists of two main tasks</p> <p>a/ Pretext task</p> <ul> <li>Stage 01: Using the unlabeled dataset (\\(x\\)), apply augmentation techniques (AG') as random crop resize and color distortion, gaussian blue to create two views, view1 (\\(x_1\\)) and view2 (\\(x_2\\)). Positive pair are \\((x_1, x_2)\\).</li> <li>Stage 02: Leverage the pre-trained model like ResNet50, ResNet101, VGG19 and InceptionResNetv2 on IMAGENET dataset as feature extractor for augmented dataset obtain the feature map \\((h_1, h_2)\\). </li> <li>Stage 03: In this stage, a linear layer is applied (MLP') as a projection from the feature map to the embedding space. Use 32/64/128 as the output dimensions of the projection for experimentation.</li> <li>Stage 04: Training the model using the transformed dataset in the embedding space, leveraging NT-Xent loss to optimizer positive pairs and maximize negative pair. After the training process finished, retain the base encoder and throw away (MLP').</li> <li>Stage 05: Finally, evaluate the model's performance by using a linear classifier (linear evaluation protocol) and training it on labeled data.</li> </ul> <p>b/ Downstream task</p> <ul> <li>Stage 01: Choose the models with the best performance after being evaluated in task 1, then fine-tuning these on labeled dataset (\\(x, y\\)) for classification task.</li> <li>Stage 02: Evaluate the model's performance using classification metrics such as Precicion, Recall and F1-score. </li> </ul>"},{"location":"research-logs/ccc-notes/note02/#ii-nt-xent-loss","title":"II. NT-Xent Loss","text":"<p>a/ Explanation and formula</p> <p>The SimCLR model uses NT-Xent loss (normalized temperature scaled cross entropy loss). We have a data point set \\(x = [x_1, x_2,\\dots, x_n]\\). By default, the SimCLR model generates two versions of each input data point using augmentation methods. As results we will have 2N data points. Suppose a data point \\(x_1\\) is augmented into two versions \\(x_i\\) and \\(x_j\\), which considered a positive pair. Then the loss function of examples \\((x_i, x_j)\\) is defined as: $$ l(i, j) = l(j, i) = -\\log \\frac{exp(sim(z_i, z_j)) / \\tau}{\\sum_{k=1}^{2N} \\mathbb{1}_{[k \\neq i]}exp(sim(z_i, z_k)) / \\tau} $$</p> <p>Then, the loss function for all data points is: $$ \\mathcal{L} = \\frac{1}{2N} \\sum_{k=1}^{N} \\left[l(2k-1, 2k) + l(2k, 2k-1) \\right] $$</p> <p>Where:</p> <ul> <li>\\(z_i, z_j\\): outputs of \\(x_i, x_j\\) after feature extracted by the projection head.</li> <li>\\(sim(z_i, z_j)\\): represents the cosine similarity between these vectors.</li> <li>\\(\\tau\\): temperature parameter.</li> </ul> <p>(+) The goal of NT-Xent loss : NT-Xent loss optimizers the unsupervised model by learning strong representations, where points from the same class are grouped closely together, while points from different classes are clearly separated. </p> <p>The \\(\\tau\\) parameter plays a crucial role in controlling how the model distinguishes between positive and negative pairs, and how it learns embeddings.</p> <ul> <li>When \\(\\tau\\) small, the model will put more focus on pushing negative pairs further apart, leading to clearer separation between positive and negative pairs.</li> <li>When \\(\\tau\\) larger, the model becomes less focused on the negative pairs, helping the model to learn more flexible representations, but may reduce its ability to precisely distinguish between negative pairs. </li> </ul> <p>b/ Manual calculation example</p> <p>Suppose we have a data set \\(x = [x_1, x_2]\\) as inputs. With batch size of 2, after applying augmentation techniques, two augmented views are generated: \\(v_1 = [x_{1i}, x_{2i}]\\) and \\(v_2 = [x_{1j}, x_{2j}]\\), where each view represents a different augmentation of the corresponding input data. </p> <p>Positive pairs is \\((x_{1i}, x_{1j})\\); \\((x_{2i}, x_{2j})\\). Below, cosine similarity matrix of all the examples.</p> \\(x_{1i}\\) \\(x_{2i}\\) \\(x_{1j}\\) \\(x_{2j}\\) \\(x_{1i}\\) 1 0.63 0.77 0.70 \\(x_{2i}\\) 0.63 1 0.67 0.84 \\(x_{1j}\\) 0.77 0.67 1 0.64 \\(x_{2j}\\) 0.70 0.84 0.64 1 <ul> <li>Stage 01: Filter the value cosine similarity of positive pairs: \\((x_{1i}, x_{1j}) = (x_{1j}, x_{1i}) =0.77\\), \\((x_{2i}, x_{2j}) = (x_{2j}, x_{2i}) = 0.84\\) and throw away the diagonal of cosine similarity matrix are always 1. With \\(\\tau = 1\\), a new cosine similarity matrix is created below.</li> </ul> \\(x_{1i}\\) \\(x_{2i}\\) \\(x_{1j}\\) \\(x_{2j}\\) \\(x_{1i}\\) - 0.63 - 0.70 \\(x_{2i}\\) 0.63 - 0.67 - \\(x_{1j}\\) - 0.67 - 0.64 \\(x_{2j}\\) 0.70 - 0.64 - <ul> <li>Stage 02: Combine the positive pairs and negative pairs. </li> </ul> \\(x_{1i}\\) \\(x_{2i}\\) \\(x_{1j}\\) \\(x_{2j}\\) \\((x_{1i}, x_{1j})=0.77\\) - 0.63 - 0.70 \\((x_{1j}, x_{1i})=0.77\\) 0.63 - 0.67 - \\((x_{2i}, x_{2j})=0.84\\) - 0.67 - 0.64 \\((x_{2j}, x_{2i})=0.84\\) 0.70 - 0.64 - <ul> <li>Stage 03: Applying the softmax function to transform the cosine similarity values into probabilities.Consider the positive pairs as label 0, and the rest as label 1. </li> </ul> \\[ S(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} \\] <p>Where:</p> <ul> <li>\\(e\\): is represents the exponential of the logit</li> <li>\\(K\\): is the number of classes</li> </ul> <p>Compute the softmax values for each row in the matrix.</p> <p>=&gt; \\(S_{r1} = \\left [\\frac{e^{0.77}}{e^{0.63} + e^{0.77} + e^{0.70}}; \\frac{e^{0.63}}{e^{0.63} + e^{0.77} + e^{0.70}}; \\frac{e^{0.7}}{e^{0.63} + e^{0.77} + e^{0.70}} \\right] = [0.3569; 0.3103; 03328]\\)</p> <p>Similarly, compute the softmax values for the remaining rows in the matrix. The vector \\([0.3569; 0.3103; 0.3328]\\) consists of softmax values, with the first element being the probability of the positive pair. Therefore, by applying cross-entropy, we can calculate the loss for each positive pair.</p> <p>=&gt; \\(l(x_{1i}, x_{1j}) = -\\log(0.3569) = -1.0303\\)</p> <p>Conduct computation of the loss for the remaining positive pairs. Finally, the loss function on the batch is:</p> <p>=&gt; \\(\\mathcal{L} = \\frac{1}{4} \\left[l(x_{1i}, x_{1j}) + l(x_{1j}, x_{1i}) + l(x_{2i}, x_{2j}) + l(x_{2j}, x_{2i})   \\right]\\)</p>"},{"location":"research-logs/ccc-notes/note02/#iii-lars-optimizer","title":"III. LARS Optimizer","text":"<p>(+) What is LARS optimizer ?</p> <p>LARS (Layer-wise Adaptive Rate Scaling) is an optimization algorithm, specifically designed for the training large-scale deep learning models, particularly in scenarios involving large-batch training. It addresses issues related to non-uniform learning rates across layers, ensuring faster convergence and improved stability during training.</p> <p>When the training large-scale models with large-batch, the ratio between the norm of weights and gradients varies significantly across layers. Besides slow or unstable convergence. To address these problems, LARS introduces layer-wise learning rate, which are adapted for layer based on the relative magnitudes of the weights and gradients. </p> <p>(+) LARS optimizer works</p> <p>At each layer \\(l\\), the weights of the model updated by LARS as follows: $$ \\Delta w_{t}^{l} = \\gamma \\cdot \\lambda^{l} \\cdot \\nabla L(w_{t}^{l}) $$</p> <p>where \\(\\gamma\\) is a global learning rate and \\(\\lambda^{l}\\) is a local learning rate, defined for each layer through \\(\\eta\\) (trust coefficient \\(\\eta\\) &lt; 1): $$ \\lambda^{l} = \\eta \\cdot \\frac{||w^l||}{||\\nabla L(w^l)||} $$</p> <p>LARS can be used to balance the local learning rate and the weight decay term \\(\\beta\\), and then applied to the update process, then \\(\\lambda^{l}\\) is defined: $$ \\lambda^{l} = \\frac{||w^{l}||}{||\\nabla L(w^l)|| + \\beta \\cdot ||w^{l}||} $$</p> <p>Momentum helps prevent the weights from updating too rapidly and reduces oscillations during optimization. The momentum for layer \\(l\\) at step \\(t+1\\) is updated as follows: $$ v_{t+1}^{l} = \\mu \\cdot v_{t}^{l} + \\gamma_{t+1} \\cdot \\lambda^{l} \\cdot (||\\nabla L(w_{t}^{l})|| + \\beta w_{t}^{l}) $$</p> <p>Where:</p> <ul> <li>\\(v_{t+1}^{l}\\) is the momentum at step \\(t+1\\) for layer \\(l\\).</li> <li>\\(\\mu\\) is the momentum factor, typically between 0 and 1.</li> <li>\\(v_{t}^{l}\\) is the momentum at the current step \\(t\\) for layer \\(l\\).</li> <li>\\(\\gamma_{t+1}\\) is the local learning rate at step \\(t+1\\).</li> <li>\\(\\nabla L(w_{t}^{l})\\) is the gradient of the loss \\(L\\) with respect to the weights \\(w_{t}^{l}\\) at step \\(t\\).</li> <li>\\(\\beta w_{t}^{l}\\) is the weight decay term (L2 regularization).</li> </ul> <p></p>"},{"location":"research-logs/ccc-notes/note03/","title":"K\u1ebf ho\u1ea1ch tri\u1ec3n khai th\u1ef1c nghi\u1ec7m v\u00e0 \u0111\u00e1nh gi\u00e1 k\u1ebft qu\u1ea3 m\u00f4 h\u00ecnh SimCLR.","text":""},{"location":"research-logs/ccc-notes/note03/#noi-dung","title":"N\u1ed9i dung","text":"<p>I. B\u1ed9 d\u1eef li\u1ec7u</p> <p>II. Thi\u1ebft k\u1ebf m\u00f4 h\u00ecnh</p> <p>III. Chi\u1ebfn l\u01b0\u1ee3c th\u1ef1c nghi\u1ec7m</p> <p>IV. \u0110\u00e1nh gi\u00e1 k\u1ebft qu\u1ea3</p> <p>V. Li\u00ean k\u1ebft nhanh</p> <p>VI. T\u00e0i li\u1ec7u tham kh\u1ea3o</p>"},{"location":"research-logs/ccc-notes/note03/#i-bo-du-lieu","title":"I. B\u1ed9 d\u1eef li\u1ec7u","text":"<p>1. M\u00f4 t\u1ea3 b\u1ed9 d\u1eef li\u1ec7u</p> <p>S\u1eed d\u1ee5ng b\u1ed9 d\u1eef li\u1ec7u Ung th\u01b0 c\u1ed5 t\u1eed cung c\u1ee7a b\u1ec7nh vi\u1ec7n A - Th\u00e1i Nguy\u00ean, bao g\u1ed3m 5 nh\u00e3n ch\u00ednh v\u1edbi s\u1ed1 l\u01b0\u1ee3ng l\u00e0 22.434 \u1ea3nh. \u0110\u00e2y l\u00e0 5 l\u1edbp nh\u00e3n b\u1ec7nh ph\u1ed5 bi\u1ebfn c\u1ee7a Ung th\u01b0 c\u1ed5 t\u1eed cung, \u0111\u01b0\u1ee3c th\u1ed1ng k\u00ea m\u00f4 t\u1ea3 \u1edf b\u1ea3ng d\u01b0\u1edbi \u0111\u00e2y.</p> Label Size ASC_H 5.669 ASC_US 3.869 HSIL 6.355 LSIL 4.780 SCC 1.761 <p>2. Chi\u1ebfn l\u01b0\u1ee3c chia d\u1eef li\u1ec7u</p> <p>B\u1ed9 d\u1eef li\u1ec7u ban \u0111\u1ea7u \u0111\u01b0\u1ee3c chia th\u00e0nh 3 t\u1eadp ri\u00eang bi\u1ec7t l\u00e0 train/dev/test v\u1edbi t\u1ef7 l\u1ec7 l\u1ea7n l\u01b0\u1ee3t l\u00e0 70%/15%/15%. M\u1ed7i b\u1ed9 d\u1eef li\u1ec7u n\u00e0y \u0111\u01b0\u1ee3c t\u00e1ch bi\u1ec7t \u0111\u1ec3 \u0111\u1ea3m b\u1ea3o t\u00ednh c\u00f4ng b\u1eb1ng trong hu\u1ea5n luy\u1ec7n v\u00e0 \u0111\u00e1nh gi\u00e1. C\u00e1c t\u1eadp n\u00e0y s\u1ebd \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng m\u1ed9t c\u00e1ch linh ho\u1ea1t trong c\u00e1c giai \u0111o\u1ea1n, chi\u1ebfn l\u01b0\u1ee3c th\u1ef1c nghi\u1ec7m, \u0111\u01b0\u1ee3c \u0111\u1ec1 c\u1eadp \u1edf Ph\u1ea7n III. </p>"},{"location":"research-logs/ccc-notes/note03/#ii-thiet-ke-mo-hinh","title":"II. Thi\u1ebft k\u1ebf m\u00f4 h\u00ecnh","text":"<p>1. M\u00f4 h\u00ecnh SimCLR (pretraining).</p> <ul> <li> <p>Backbone : S\u1eed d\u1ee5ng c\u00e1c m\u00f4 h\u00ecnh h\u1ecdc s\u00e2u \u0111a d\u1ea1ng, c\u00e2n b\u1eb1ng gi\u1eefa t\u1ed1c \u0111\u1ed9 v\u00e0 \u0111\u1ed9 ch\u00ednh x\u00e1c, ph\u00f9 h\u1ee3p cho tr\u00edch xu\u1ea5t \u0111\u1eb7c tr\u01b0ng t\u1eeb \u1ea3nh y t\u1ebf nh\u01b0: ResNet-50, ResNet-101, DenseNet-121, Inception-V3, EfficientNet-B0.</p> </li> <li> <p>MLP layers : S\u1eed d\u1ee5ng l\u00e0m \u0111\u1ea7u chi\u1ebfu (projection head), sau khi c\u00e1c backbone t\u1ea1o ra vector \u0111\u1eb7c tr\u01b0ng, ch\u00fang s\u1ebd \u0111\u01b0\u1ee3c \u0111\u01b0a qua m\u1ed9t MLP nh\u1ecf (th\u01b0\u1eddng g\u1ed3m 2 \u0111\u1ebfn 3 l\u1edbp fully connected + h\u00e0m k\u00edch ho\u1ea1t ReLU). M\u1ee5c \u0111\u00edch l\u00e0 bi\u1ebfn \u0111\u1ed5i c\u00e1c embedding h\u00ecnh \u1ea3nh v\u00e0o m\u1ed9t kh\u00f4ng gian m\u1edbi \u0111\u1ec3 t\u00ednh to\u00e1n loss.</p> </li> <li> <p>Projection head: \u0110\u00e2y l\u00e0 k\u00edch th\u01b0\u1edbc \u0111\u1ea7u ra t\u00f9y ch\u1ec9nh sau khi \u00e1p d\u1ee5ng MLP layers cho c\u00e1c vector \u0111\u1eb7c tr\u01b0ng \u0111\u01b0\u1ee3c tr\u00edch xu\u1ea5t t\u1eeb backbone. Th\u00f4ng th\u01b0\u1eddng c\u00e1c vector \u0111\u1eb7c tr\u01b0ng s\u1ebd \u0111\u01b0\u1ee3c chuy\u1ec3n sang m\u1ed9t kh\u00f4ng gian bi\u1ec3u di\u1ebfn m\u1edbi c\u00f3 k\u00edch th\u01b0\u1edbc nh\u1ecf h\u01a1n (128 ho\u1eb7c 256).</p> </li> </ul> M\u00f4 h\u00ecnh Backbone MLP Layers Projection Head ResNet-50 DenseNet-121 SimCLR Inception-V3 2 (layer) 128 (output_dim) ResNet-101 EfficientNet-B0 <p>2. M\u00f4 h\u00ecnh baseline.</p> <p>Ti\u1ebfn h\u00e0nh thi\u1ebft l\u1eadp m\u1ed9t s\u1ed1 m\u00f4 h\u00ecnh \u0111\u01a1n gi\u1ea3n ban \u0111\u1ea7u nh\u1eb1m so s\u00e1nh hi\u1ec7u qu\u1ea3 v\u1edbi c\u00e1c m\u00f4 h\u00ecnh h\u1ecdc representation (bi\u1ec3u di\u1ec5n) nh\u01b0 SimCLR. L\u1ef1a ch\u1ecdn 5 m\u00f4 h\u00ecnh l\u00e0m backbone cho SimCLR (nh\u01b0 \u0111\u00e3 \u0111\u1ec1 c\u1eadp \u1edf ph\u00eda tr\u00ean) l\u00e0m c\u00e1c m\u00f4 h\u00ecnh baseline.</p>"},{"location":"research-logs/ccc-notes/note03/#iii-chien-luoc-thuc-nghiem","title":"III. Chi\u1ebfn l\u01b0\u1ee3c th\u1ef1c nghi\u1ec7m","text":"<p>1. Hu\u1ea5n luy\u1ec7n SimCLR</p> <p>Giai \u0111o\u1ea1n 1: Pretraining SimCLR:</p> <ul> <li>Th\u1ef1c hi\u1ec7n pretraining SimCLR cho b\u1ed9 d\u1eef li\u1ec7u train + dev (\u0111\u00e3 \u0111\u01b0\u1ee3c ph\u00e2n chia \u1edf Ph\u1ea7n I) v\u00e0 kh\u00f4ng s\u1eed d\u1ee5ng nh\u00e3n.</li> <li>S\u1eed d\u1ee5ng h\u00e0m m\u1ea5t m\u00e1t NT-Xent \u0111\u1ec3 h\u1ecdc bi\u1ec3u di\u1ec5n.</li> <li>Hu\u1ea5n luy\u1ec7n SimCLR l\u1ea7n l\u01b0\u1ee3t v\u1edbi 5 backbone \u0111\u00e3 \u0111\u01b0\u1ee3c l\u1ef1a ch\u1ecdn.</li> </ul> Model (backbone) Augmentation Loss Optimizer (learning rate) Batch Size Epoch SimCLR (ResNet-50) SimCLR (ResNet-101) SimCLR (DenseNet-121) (1) NT-Xent Adam(lr=0.3) 128 100 SimCLR (Inception-V3) SimCLR (EfficientNet-B0) <p>(1) C\u00e1c k\u1ef9 thu\u1eadt augmentation \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng bao g\u1ed3m: RandomResizedCrop, RandomHorizontalFlip, RandomApply, RandomGrayScale, GaussianBlur, Normalize. </p> <p>Giai \u0111o\u1ea1n 2: Fine-tuning (Linear protocol)</p> <ul> <li> <p>\u0110\u1ec3 \u0111\u00e1nh gi\u00e1 kh\u1ea3 n\u0103ng t\u1eadn d\u1ee5ng d\u1eef li\u1ec7u kh\u00f4ng g\u00e1n nh\u00e3n c\u1ee7a m\u00f4 h\u00ecnh SimCLR, ti\u1ebfn h\u00e0nh fine-tune m\u00f4 h\u00ecnh v\u1edbi t\u1eadp d\u1eef li\u1ec7u train (s\u1eed d\u1ee5ng nh\u00e3n).</p> </li> <li> <p>Fine-tune pretrained SimCLR l\u1ea7n l\u01b0\u1ee3t v\u1edbi t\u1ef7 l\u1ec7 d\u1eef li\u1ec7u c\u00f3 nh\u00e3n (5, 10, 30, 50, 70, 90, 100%). M\u1ee5c ti\u00eau nh\u1eb1m x\u00e1c \u0111\u1ecbnh t\u00ednh hi\u1ec7u qu\u1ea3 c\u1ee7a SimCLR khi s\u1ed1 l\u01b0\u1ee3ng nh\u00e3n gi\u1ea3m m\u1ea1nh.</p> </li> </ul> Model (backbone) Augmentation Loss Optimizer (leanring rate) Batch Size Epoch SimCLR (ResNet-50) SimCLR (ResNet-101) SimCLR (DenseNet-121) (2) Cross-Entropy Adam(lr=1e-3) 64 100 SimCLR (Inception-V3) SimCLR (EfficientNet-B0) <p>(2) C\u00e1c k\u1ef9 thu\u1eadt augmentation \u1edf \u0111\u00e2y \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng bao g\u1ed3m: Resize, CenterCrop, RandomHorizontalFlip, Normalize.</p> <p>2. Hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh baseline</p> <ul> <li>S\u1eed d\u1ee5ng to\u00e0n b\u1ed9 t\u1eadp train (c\u00f3 nh\u00e3n) \u0111\u1ec3 fine-tune tr\u00ean t\u1ea5t c\u1ea3 c\u00e1c m\u00f4 h\u00ecnh baseline, vi\u1ec7c setup fine-tune nh\u01b0 augmentation, loss, optimizer, batch size gi\u1ed1ng h\u1ec7t nh\u01b0 giai \u0111o\u1ea1n fine-tune pretrained SimCLR.</li> <li>Fine-tune c\u00f3 \u0111i\u1ec1u ch\u1ec9nh v\u1edbi t\u1ef7 l\u1ec7 d\u1eef li\u1ec7u c\u00f3 nh\u00e3n kh\u00e1c nhau (t\u01b0\u01a1ng t\u1ef1 fine-tune pretrained SimCLR), \u0111\u1ea3m b\u1ea3o t\u00ednh c\u00f4ng b\u1eb1ng khi so s\u00e1nh hi\u1ec7u su\u1ea5t gi\u1eefa hai chi\u1ebfn l\u01b0\u1ee3c supervised learning v\u00e0 self-supervised learning. </li> </ul> <p>Ghi ch\u00fa : </p> <ul> <li>Trong qu\u00e1 tr\u00ecnh pretraining SimCLR, vi\u1ec7c s\u1eed d\u1ee5ng c\u1ea3 t\u1eadp train + dev (kh\u00f4ng nh\u00e3n) l\u00e0m d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n gi\u00fap t\u1eadn d\u1ee5ng t\u1ed1i \u0111a d\u1eef li\u1ec7u s\u1eb5n c\u00f3.</li> <li>Qu\u00e1 tr\u00ecnh fine-tune pretrained SimCLR v\u00e0 m\u00f4 h\u00ecnh baseline s\u1eed d\u1ee5ng t\u1eadp train (c\u00f3 nh\u00e3n) l\u00e0m d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n v\u00e0 t\u1eadp dev (c\u00f3 nh\u00e3n) l\u00e0m t\u1eadp ki\u1ec3m \u0111\u1ecbnh, nh\u1eb1m ch\u1ecdn ra m\u00f4 h\u00ecnh t\u1ed1t nh\u1ea5t.</li> <li>T\u1eadp test (c\u00f3 nh\u00e3n) ch\u1ec9 \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng m\u1ed9t l\u1ea7n duy nh\u1ea5t sau khi qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n v\u00e0 l\u1ef1a ch\u1ecdn m\u00f4 h\u00ecnh ho\u00e0n t\u1ea5t. \u0110\u1ea3m b\u1ea3o t\u00ednh kh\u00e1ch quan v\u00e0 \u0111\u1ed9 tin c\u1eady c\u1ee7a th\u1ef1c nghi\u1ec7m.</li> <li>T\u1ea5t c\u1ea3 c\u00e1c m\u00f4 h\u00ecnh SimCLR v\u00e0 baseline \u0111\u1ec1u kh\u1edfi t\u1ea1o b\u1eb1ng tr\u1ecdng s\u1ed1 ImageNet cho qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n, \u0111\u1ea3m b\u1ea3o r\u1eb1ng s\u1ef1 kh\u00e1c bi\u1ec7t hi\u1ec7u su\u1ea5t kh\u00f4ng \u0111\u1ebfn t\u1eeb ph\u01b0\u01a1ng ph\u00e1p hu\u1ea5n luy\u1ec7n (kh\u00f4ng ph\u1ea3i do l\u1ee3i th\u1ebf kh\u1edfi t\u1ea1o kh\u00e1c nhau).</li> <li>S\u1eed d\u1ee5ng chi\u1ebfn l\u01b0\u1ee3c hu\u1ea5n luy\u1ec7n E2E (End-to-End Fine-tuning) \u0111\u1ec3 tinh ch\u1ec9nh to\u00e0n b\u1ed9 m\u00f4 h\u00ecnh t\u1eeb \u0111\u1ea7u, TS (Two-Stage Fine-tuning) \u0111\u1ec3 hu\u1ea5n luy\u1ec7n hai giai \u0111o\u1ea1n. C\u1ea3 hai chi\u1ebfn l\u01b0\u1ee3c n\u00e0y s\u1ebd \u0111\u01b0\u1ee3c \u00e1p d\u1ee5ng cho c\u00e1c giai \u0111o\u1ea1n fine-tune c\u1ee7a to\u00e0n b\u1ed9 th\u1ef1c nghi\u1ec7m. </li> </ul>"},{"location":"research-logs/ccc-notes/note03/#iv-anh-gia-ket-qua","title":"IV. \u0110\u00e1nh gi\u00e1 k\u1ebft qu\u1ea3","text":"<p>1. \u0110\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh SimCLR</p> <p>a/ \u0110\u00e1nh gi\u00e1 giai \u0111o\u1ea1n pretrained SimCLR</p> M\u00f4 h\u00ecnh Backbone Weight NT-Xent Loss ResNet-50 Random init ImageNet DenseNet-121 Random init ImageNet SimCLR Inception-V3 Random init ImageNet ResNet-101 Random init ImageNet EfficientNet-B0 Random init ImageNet <p></p>"},{"location":"research-logs/ccc-notes/note04/","title":"Vision Language Model - VLM","text":""},{"location":"research-logs/ccc-notes/note04/#noi-dung","title":"N\u1ed9i dung","text":"<ul> <li>I. M\u00f4 h\u00ecnh CLIP</li> <li>IV. M\u00f4 h\u00ecnh BLIP</li> </ul>"},{"location":"research-logs/ccc-notes/note04/#i-mo-hinh-clip","title":"I. M\u00f4 h\u00ecnh CLIP","text":"<p>1. Overview</p> <p>Information: The CLIP model was introduced in Jan, 2020 by OpenAI with paper title \"Learning Transferable Visual Models from Natural Language Supervision\". This is a highlight architecture in combining language and image learning, opening up zero-shot learning for a wide range of computer vision tasks.</p> <p>Objectives of Model: </p> <ul> <li> <p>Training the model to predict which caption matches which image on a dataset of 400 million pair (image, text), developed an efficient image representation learning model from scratch. </p> </li> <li> <p>Then the training, can use natural language to refer to or describe visual concepts, allowing zero-shot transfer to a variety of tasks.</p> </li> </ul> <p>Results achieved: The model was evaluated on more than 30 datasets: character recognition, video action, geolocation and fine-grained object classification. Model performed competitively with supervised training methods, accuracy comparable to ResNet-50 on ImageNet without using its training set.</p> <p>2. Architecture model</p> <p>Image encoder: Use ResNet50, ResNetD and ViT as the base architecture for the image encoder. Replace the GAP layer with an attention pooling mechanism - transformer style (multi-head QKV). </p> <p>Text encoder: </p> <ul> <li> <p>Transformer with the architecture modifications: 63M-parameter, 12-layer 512-wide model with 8 attention heads. </p> </li> <li> <p>Text converted to token by BPE with a 49.152 vocab size and sequence length at 76.</p> </li> </ul> <p>Multi-model embedding space: Both image feature and text feature are layer normalized, then linearly projected into the multi-model embedding space to calculate the similarity between images and descriptions.</p> <p>3. Zero-shot transfer</p> <p>Progress:</p> <ol> <li>Use image encoder to get image embedding, use text encoder to get embeddings for all class names.</li> <li>CLIP computes cosine similarities between the image and text embeddings, scales them by a temperature parameter, then applies softmax to get probabilities. This works like a softmax classifier where both the inputs and class embeddings are L2-normalized, there's no bias term, and temperature controls the sharpness of the output. </li> </ol> <p>Multinomial logistic regression classifier:</p> \\[\\text{logit}_{i} = f_\\text{img} \\cdot f_\\text{text, i}\\] <p>where:</p> <ul> <li>\\(f_\\text{img}\\) is embedding image / inputs</li> <li>\\(f_\\text{text, i}\\) is embedding of text \\(i\\) / weights</li> <li>\\(\\text{logit}_{i}\\) is cosine similarity. </li> </ul> <p>Devide the logits by a temperature (\\(\\tau\\)) coefficient. Then pass it through softmax to get the classification probabilities.</p> \\[P_{i} = \\frac{e^{logit_i / \\tau}}{\\sum_{j}e^{logit_j / \\tau}}\\] <p>3. Evaluation zero-shot CLIP</p> <p>Objectives: The main goal is to evaluate the quality of the representation learned by CLIP during its large-scale pre-training.</p> <p>More specifically:</p> <ul> <li>Logistic Regression = Supervised baseline:<ul> <li>Logistic Regression is trained features extracted from a standard backbone (ResNet50, v.v), using labeled training.</li> <li>It represents a simple and standard supervised learning baseline, commonly used to evaluate learned representations. </li> </ul> </li> <li>CLIP zero-shot = No training on the new datasets:<ul> <li>CLIP doesn't require fine-tuning or labels from the new dataset.</li> <li>It simply matches image features with text embedding using cosine similarity.</li> <li>Predictions is done directly using \u00eds knowledge CLIP learned during pre-training.</li> </ul> </li> </ul> <p>4. Limitations</p> <p>CLIP also struggles on some tasks, especially:</p> <ul> <li>Fine-grained classification, like telling apart car models, flower species, or airplane types.</li> <li>Abstract or systematic tasks, like counting objects in an image.</li> <li>New or uncommon tasks that probably weren't in CLIP's training set. </li> </ul>"},{"location":"research-logs/ccc-notes/note04/#ii-mo-hinh-blip","title":"II. M\u00f4 h\u00ecnh BLIP","text":"<p>1. T\u1ed5ng quan</p> <p>Information: BLIP (Bootstrapped Language-Image Pretranining) l\u00e0 m\u1ed9t m\u00f4 h\u00ecnh \u0111a ph\u01b0\u01a1ng th\u1ee9c (muiltimodal) \u0111\u01b0\u1ee3c gi\u1edbi thi\u1ec7u v\u00e0o n\u0103m 2022 trong b\u00e0i b\u00e1o \"Bootstrapping Language-Image Pre-tranining for United Vision-Language Understanding and Generation\" b\u1edfi Saleforce Research. \u0110i\u1ec3m n\u1ed5i b\u1eadt c\u1ee7a m\u00f4 h\u00ecnh n\u00e0y l\u00e0 kh\u1ea3 n\u0103ng chuy\u1ec3n \u0111\u1ed5i linh ho\u1ea1t gi\u1eefa c\u00e1c t\u00e1c v\u1ee5 hi\u1ec3u v\u00e0 sinh ng\u00f4n ng\u1eef li\u00ean quan \u0111\u1ebfn h\u00ecnh \u1ea3nh nh\u01b0: tr\u1ea3 l\u1eddi c\u00e2u h\u1ecfi v\u1ec1 \u1ea3nh (VQA), t\u00ecm \u1ea3nh d\u1ef1a tr\u00ean m\u00f4 t\u1ea3 (retrieval), t\u1ea1o ch\u00fa th\u00edch v\u00e0 sinh m\u00f4 t\u1ea3 cho \u1ea3nh m\u1edbi.</p> <p>M\u1ee5c ti\u00eau: Cho ph\u00e9p hu\u1ea5n luy\u1ec7n v\u00e0 s\u1eed d\u1ee5ng nhi\u1ec1u task trong m\u1ed9t m\u00f4 h\u00ecnh duy nh\u1ea5t. C\u00f3 th\u1ec3 h\u1ecdc t\u1ed1t c\u1ea3 hi\u1ec3u v\u00e0 sinh v\u0103n b\u1ea3n.</p> <p>2. C\u1ea5u tr\u00fac</p> <p>Ho\u1ea1t \u0111\u1ed9ng d\u1ef1a tr\u00ean ki\u1ebfn tr\u00fac MED (Multimodal mixture of encoder-decoder) - ki\u1ebfn tr\u00fac \u0111a t\u00e1c v\u1ee5 c\u00f3 th\u1ec3 ho\u1ea1t \u0111\u1ed9ng v\u1edbi 3 ch\u1ee9c n\u0103ng ch\u00ednh:</p> <ul> <li> <p>Unimodal encoder mode (b\u1ed9 m\u00e3 h\u00f3a \u0111\u01a1n): m\u00e3 h\u00f3a ri\u00eang bi\u1ec7t cho h\u00ecnh \u1ea3nh v\u00e0 v\u0103n b\u1ea3n.</p> <ul> <li> <p>S\u1eed d\u1ee5ng ViT \u0111\u1ec3 m\u00e3 h\u00f3a h\u00ecnh \u1ea3nh, chia h\u00ecnh \u1ea3nh \u0111\u1ea7u v\u00e0o th\u00e0nh c\u00e1c m\u1ea3nh nh\u1ecf (patches), sau \u0111\u00f3 m\u00e3 h\u00f3a ti\u1ebfp c\u00e1c m\u1ea3nh n\u00e0y th\u00e0nh m\u1ed9t chu\u1ed7i nh\u00fang (embeddings).</p> </li> <li> <p>S\u1eed d\u1ee5ng BERT \u0111\u1ec3 m\u00e3 h\u00f3a cho v\u0103n b\u1ea3n (c\u00e2u h\u1ecfi, m\u00f4 t\u1ea3, v.v.).</p> </li> <li> <p>Hai ph\u01b0\u01a1ng th\u1ee9c n\u00e0y kh\u00f4ng c\u00f3 s\u1ef1 li\u00ean k\u1ebft, m\u00e0 \u0111\u01b0\u1ee3c m\u00e3 h\u00f3a ri\u00eang bi\u1ec7t. </p> </li> </ul> </li> <li> <p>Image-grounded text encoder (b\u1ed9 m\u00e3 h\u00f3a v\u0103n b\u1ea3n d\u1ef1a tr\u00ean h\u00ecnh \u1ea3nh): t\u00edch h\u1ee3p th\u00f4ng tin h\u00ecnh \u1ea3nh v\u00e0o qu\u00e1 tr\u00ecnh m\u00e3 h\u00f3a v\u0103n b\u1ea3n.</p> <ul> <li> <p>Ch\u00e8n m\u1ed9t cross-attention b\u1ed5 sung v\u00e0o gi\u1eefa l\u1edbp self-attention v\u00e0 FFN trong m\u1ed7i kh\u1ed1i transformer c\u1ee7a b\u1ed9 m\u00e3 h\u00f3a v\u0103n b\u1ea3n. L\u1edbp cross-attention cho ph\u00e9p m\u00f4 h\u00ecnh ch\u00fa \u00fd \u0111\u1ebfn c\u00e1c \u0111\u1eb7c tr\u01b0ng h\u00ecnh \u1ea3nh khi x\u1eed l\u00fd v\u0103n b\u1ea3n.</p> </li> <li> <p>M\u1ed9t token [Encode] \u0111\u01b0\u1ee3c n\u1ed1i v\u00e0o v\u0103n b\u1ea3n. \u0110\u1ea7u ra c\u1ee7a token [Encode] l\u00e0 embedding s\u1eed d\u1ee5ng l\u00e0m bi\u1ec3u di\u1ec5n \u0111a ph\u01b0\u01a1ng th\u1ee9c cho c\u00e1c c\u1eb7p h\u00ecnh \u1ea3nh v\u00e0 v\u0103n b\u1ea3n.</p> </li> </ul> </li> <li> <p>Image-grounded text decoder (b\u1ed9 gi\u1ea3i m\u00e3 v\u0103n b\u1ea3n d\u1ef1a tr\u00ean h\u00ecnh \u1ea3nh): ch\u1ee9c n\u0103ng n\u00e0y \u0111\u01b0\u1ee3c thi\u1ebft k\u1ebf cho ph\u00e9p m\u00f4 h\u00ecnh sinh v\u0103n b\u1ea3n d\u1ef1a tr\u00ean th\u00f4ng tin t\u1eeb h\u00ecnh \u1ea3nh.</p> <ul> <li> <p>Thay v\u00ec s\u1eed d\u1ee5ng self-attention hai chi\u1ec1u nh\u01b0 m\u00f4 h\u00ecnh BERT, s\u1eed d\u1ee5ng causal attention gi\u1ed1ng nh\u01b0 GPT, khi t\u1ea1o ra m\u1ed9t t\u1eeb m\u1edbi, m\u00f4 h\u1ec9nh ch\u1ec9 c\u00f3 th\u1ec3 ch\u00fa \u00fd \u0111\u1ebfn c\u00e1c t\u1eeb \u0111\u00e3 \u0111\u01b0\u1ee3c t\u1ea1o tr\u01b0\u1edbc \u0111\u00f3.</p> </li> <li> <p>M\u1ed9t token [Decode] \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 b\u00e1o hi\u1ec7u s\u1ef1 b\u1eaft \u0111\u1ea7u c\u1ee7a m\u1ed9t chu\u1ed7i, v\u00e0 k\u1ebft th\u00fac chu\u1ed7i b\u1eb1ng [EST] (end-of-sequence token). </p> </li> </ul> </li> </ul> <p>Qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n tr\u01b0\u1edbc c\u1ee7a m\u00f4 h\u00ecnh BLIP d\u1ef1a tr\u00ean vi\u1ec7c t\u1ed1i \u01b0u h\u00f3a \u0111\u1ed3ng th\u1eddi ba m\u1ee5c ti\u00eau: hai m\u1ee5c ti\u00eau t\u1eadp trung v\u00e0o kh\u1ea3 n\u0103ng hi\u1ec3u v\u00e0 m\u1ed9t m\u1ee5c ti\u00eau t\u1eadp trung v\u00e0o kh\u1ea3 n\u0103ng t\u1ea1o sinh.</p> <ul> <li> <p>Image-Text Contrastive Loss (ITC): nhi\u1ec7m v\u1ee5 n\u00e0y s\u1ebd k\u00edch ho\u1ea1t ch\u1ebf \u0111\u1ed9 unimodal encoder.</p> <ul> <li> <p>M\u00e3 h\u00f3a h\u00ecnh \u1ea3nh v\u00e0 v\u0103n b\u1ea3n m\u1ed9t c\u00e1ch ri\u00eang bi\u1ec7t sau \u0111\u00f3 \u0111\u01b0a v\u00e0o m\u1ed9t kh\u00f4ng gian chung. \u1ede \u0111\u00e2y m\u1ed9t h\u00e0m loss \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng nh\u1eb1m k\u00e9o c\u00e1c c\u1eb7p image-caption d\u01b0\u01a1ng t\u00ednh l\u1ea1i g\u1ea7n nhau (t\u1ee9c kh\u1edbp nhau) v\u00e0 \u0111\u1ea9y c\u00e1c c\u1eb7p \u00e2m t\u00ednh xa nhau.</p> </li> <li> <p>C\u1ea3i thi\u1ec7n kh\u1ea3 n\u0103ng hi\u1ec3u v\u1ec1 th\u1ecb gi\u00e1c v\u00e0 ng\u00f4n ng\u1eef, c\u0103n ch\u1ec9nh kh\u00f4ng gian bi\u1ec3u di\u1ec5n c\u1ee7a \u1ea3nh v\u00e0 v\u0103n b\u1ea3n.</p> </li> </ul> </li> <li> <p>Image-Text Matching Loss (ITM): K\u00edch ho\u1ea1t ch\u1ebf \u0111\u1ed9 image-grounded text encoder.</p> <ul> <li> <p>H\u1ecdc m\u1ed9t bi\u1ec3u di\u1ec5n \u0111a ph\u01b0\u01a1ng th\u1ee9c g\u1ed3m h\u00ecnh \u1ea3nh v\u00e0 v\u0103n b\u1ea3n, c\u00f3 kh\u1ea3 n\u0103ng n\u1eafm b\u1eaft \u0111\u01b0\u1ee3c s\u1ef1 li\u00ean k\u1ebft chi ti\u1ebft gi\u1eefa hai d\u1eef li\u1ec7u n\u00e0y.</p> </li> <li> <p>S\u1eed d\u1ee5ng m\u1ed9t l\u1edbp tuy\u1ebfn t\u00ednh \u0111\u01a1n gi\u1ea3n k\u1ebft h\u1ee3p v\u1edbi ph\u00e2n lo\u1ea1i nh\u1ecb ph\u00e2n, nh\u1eb1m d\u1ef1 \u0111o\u00e1n xem m\u1ed9t c\u1eb7p h\u00ecnh \u1ea3nh-v\u0103n b\u1ea3n l\u00e0 d\u01b0\u01a1ng t\u00ednh hay \u00e2m t\u00ednh d\u1ef1a tr\u00ean c\u00e1c \u0111\u1eb7c tr\u01b0ng \u0111a ph\u01b0\u01a1ng th\u1ee9c c\u1ee7a ch\u00fang.</p> </li> </ul> </li> <li> <p>Language Modeling Loss (LM): K\u00edch ho\u1ea1t ch\u1ebf \u0111\u1ed9 image-grounded text encoder.</p> <ul> <li> <p>T\u1ea1o ra c\u00e1c m\u00f4 t\u1ea3 v\u0103n b\u1ea3n d\u1ef1a tr\u00ean m\u1ed9t h\u00ecnh \u1ea3nh cho tr\u01b0\u1edbc. </p> </li> <li> <p>T\u1ed1i \u01b0u h\u00f3a h\u00e0m m\u1ea5t m\u00e1t cross-entropy, gi\u00fap t\u1ed1i \u0111a h\u00f3a x\u00e1c xu\u1ea5t c\u1ee7a v\u0103n b\u1ea3n theo ki\u1ec3u t\u1ef1 h\u1ed3i quy, d\u1ef1 \u0111o\u00e1n t\u1eeb ti\u1ebfp theo d\u1ef1a tr\u00ean c\u00e1c t\u1eeb \u0111\u00e3 \u0111\u01b0\u1ee3c t\u1ea1o ra t\u1eeb tr\u01b0\u1edbc \u0111\u00f3 v\u00e0 th\u00f4ng tin t\u1eeb h\u00ecnh \u1ea3nh.</p> </li> </ul> </li> </ul> <p>Chia s\u1ebb tham s\u1ed1: Trong qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n tr\u01b0\u1edbc, \u0111\u1ec3 \u0111\u1ea3m b\u1ea3o s\u1ef1 hi\u1ec7u qu\u1ea3 v\u00e0 t\u1eadn d\u1ee5ng \u0111\u01b0\u1ee3c l\u1ee3i \u00edch t\u1eeb h\u1ecdc \u0111a t\u00e1c v\u1ee5, text encoder v\u00e0 text decoder chia s\u1ebb t\u1ea5t c\u1ea3 c\u00e1c tham s\u1ed1 ngo\u1ea1i tr\u1eeb c\u00e1c l\u1edbp self-attention. Vi\u1ec7c n\u00e0y gi\u00fap c\u1ea3i thi\u1ec7n hi\u1ec7u qu\u1ea3 hu\u1ea5n luy\u1ec7n, gi\u1ea3m thi\u1ec3u s\u1ed1 l\u01b0\u1ee3ng tham s\u1ed1, t\u0103ng t\u1ed1c \u0111\u1ed9 trong khi c\u00e1c t\u00e1c v\u1ee5 v\u1eabn c\u00f3 th\u1ec3 h\u1ecdc h\u1ecfi v\u00e0 h\u1ed7 tr\u1ee3 l\u1eabn nhau.</p>"},{"location":"research-logs/ccc-notes/note05/","title":"T\u1ed5ng h\u1ee3p hi\u1ec7u su\u1ea5t khi fine-tune BLIP/BLIP2 tr\u00ean d\u1eef li\u1ec7u VQA","text":""},{"location":"research-logs/ccc-notes/note05/#i-bo-du-lieu","title":"I. B\u1ed9 d\u1eef li\u1ec7u","text":"STT B\u1ed9 d\u1eef li\u1ec7u M\u00f4 t\u1ea3 Ngu\u1ed3n 01 PathVQA B\u1ed9 d\u1eef li\u1ec7u g\u1ed3m 32.799 c\u00e2u h\u1ecfi \u1ec1 4.998 h\u00ecnh \u1ea3nh b\u1ec7nh l\u00fd. [Link] 02 VQA-NLE-LlaVA B\u1ed9 d\u1eef li\u1ec7u t\u1ed5ng h\u1ee3p t\u1eeb GQA v\u1edbi c\u00e2u h\u1ecfi v\u00e0 c\u00e2u tr\u1ea3 l\u1eddi k\u00e8m gi\u1ea3i th\u00edch [Link] 03 BLIP_VQA_Vietnamese B\u1ed9 d\u1eef li\u1ec7u ti\u1ebfng vi\u1ec7t v\u1edbi c\u00e2u h\u1ecfi v\u1ec1 ph\u00e1t hi\u1ec7n, \u0111\u1ebfm, nh\u1eadn di\u1ec7n trong \u1ea3nh [Link] 04 Viet-Doc-VQA-flash2 B\u1ed9 d\u1eef li\u1ec7u t\u1eeb t\u00e0i li\u1ec7u gi\u00e1o khoa v\u1edbi c\u00e2u h\u1ecfi-tr\u1ea3 l\u1eddi [Link]"},{"location":"research-logs/ccc-notes/note05/#ii-phuong-phap-thuc-nghiem-anh-gia-va-ket-qua","title":"II. Ph\u01b0\u01a1ng ph\u00e1p th\u1ef1c nghi\u1ec7m, \u0111\u00e1nh gi\u00e1 v\u00e0 k\u1ebft qu\u1ea3","text":"<p>1. Ph\u01b0\u01a1ng ph\u00e1p th\u1ef1c nghi\u1ec7m </p> <p>S\u1eed d\u1ee5ng m\u00f4 h\u00ecnh BLIP/BLIP2 fine-tune cho nhi\u1ec7m v\u1ee5 vqa.</p> <ul> <li> <p>Fine-tune c\u1ea3 hai m\u00f4 h\u00ecnh tr\u00ean 2 t\u1eadp d\u1eef li\u1ec7u ti\u1ebfng anh.</p> </li> <li> <p>S\u1eed d\u1ee5ng m\u00f4 h\u00ecnh \u0111\u00e3 \u0111\u01b0\u1ee3c fine-tune tr\u00ean hai b\u1ed9 ti\u1ebfng anh \u0111\u1ec3 fine-tune ti\u1ebfp tr\u00ean hai b\u1ed9 d\u1eef li\u1ec7u ti\u1ebfng vi\u1ec7t.</p> </li> </ul> <p>2. K\u1ebft qu\u1ea3 th\u1ef1c nghi\u1ec7m</p> B\u1ed9 D\u1eef li\u1ec7u Ph\u00e9p \u0110o BLIP BLIP-2 PathVQA Yes/No Accuracy 81.60% 90.40% EM 37.50% 39.00% VQA-NLE-LLaVA EM 41.20% 43.80% BLEU-4 25.0 27.3 ROUGE-L 30.5 32.1 BLIP_VQA_Vietnamese Accuracy 74.80% 77.10% F1 Score 78.00% 81.00% Viet-Doc-VQA-flash2 ANLS 0.65 0.68"},{"location":"resources/","title":"Hoc's Resources","text":"<p>Info</p> <p>This is repository containing all the resources related to course website, books, blog, etc., covering fields such as:</p> <ul> <li>Programming: C/C++, Python, Java, etc.</li> <li>Data Structure &amp; Algorithms</li> <li>Data Science &amp; Artificial Intelligence</li> <li>Machine Learning</li> <li>Deep Learning &amp; Computer Vision</li> <li>Natural Language Processing</li> </ul>"},{"location":"resources/#materials-for-each-topic","title":"Materials for Each Topic","text":"<ul> <li> <p>Programming Languages</p> <p>Pending...</p> <p> Getting started</p> </li> <li> <p>Data Structure &amp; Algorithms</p> <p>Pending...</p> <p> Getting started</p> </li> <li> <p>Data Science &amp; Artificial Intelligence</p> <p>Pending...</p> <p>Getting started</p> </li> <li> <p>Machine Learning</p> <p>Pending...</p> <p>Getting started</p> </li> <li> <p>Deep Learning &amp; Computer Vision</p> <p>Pending...</p> <p>Getting started</p> </li> <li> <p>Natural Language Processing &amp; Large Language Model</p> <p>Pending...</p> <p>Getting started</p> </li> </ul> <p></p>"}]}