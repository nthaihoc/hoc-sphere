---
icon: material/numeric-2-circle
---

# :fontawesome-solid-graduation-cap: Deep Learning Course
---

## I. Description

This course provides a comprehensive journey into Deep Learning, covering everything from fundamental neural network concepts to cutting-edge architecture like CNNs, LSTMs, Transformers and GAN. You will gain hands-on experience in building, training, and deploying AI models using Tensorflow/Pytorch.

## II. What You'll learn

- Neural networks, backpropagation and optimization techniques
- Convolutional Neural Networks for image processing
- Recurrent Neural Networks, Long Short Term Memory for sequential data
- Transformers & Attention mechanisms
- Generative Adversarial Networks for AI-generated content
- Hyperparameter tuning and hyperparameter methods

## III. Prerequisites

To take this course, you should have a basic understanding of Python programming, along with fundamental concepts in linear algebra, probability, statistics and machine learning. Prior experience with Tensorflow/Pytorch is helpful but not required.

## IV. Lecture Schedule

| Lecture | Title | Description | Status | Resources |
| :-----: | :---- | :---------- | :----: | :-------: |
|   01    | Introduction in Deep Learning |(-) Key concepts in Deep Learning <br> (-) Real-world applications of Deep Learning <br> (-) Overview of neural networks and their structure | :material-close:| :material-close: |
|   02    | Neural Networks and Backpropagation | (-) Neural network structure <br> (-) Activation functions <br> (-) Feedforward process <br> (-) Backpropagation & Gradient Descent | :material-close: | :material-close: |
|   03    | Loss functions and Optimizers | (-) Cross-Entropy, Huber Loss, Mean Squared Error <br> (-) Stochastic Gradient Descent, Momentum, Nesterov Accelerated Gradient <br> (-) AdaGrad, RMSProp, Adam, AdamW, LARS | :material-close: | :material-close: |
|   04    | Regularization and Batch Normalization | (-) Overfitting vs. Underfitting <br> L1 (Lasso) & L2 (Ridge) regularization, Dropout, Early stopping <br> (-) Batch Normalization | :material-close: | :material-close: |
|   05    | Advanced Neural Networks | (-) Convolutional Neural Networks <br> (-) Popular CNN architecture <br> (-) Residual connections | :material-close: | :material-close: |
|   06    | Recurrent Neural Networks & Long Short-Term Memory | (-) RNN process sequential data <br> (-) LSTM in handling long-range dependencies <br> (-) Gated Recurrent Unit (GRU) | :material-close:| :material-close: |
|   07    |  Transformers & Attention Mechanism | (-) Key concepts <br> (-) Self-Attention & Multi-Head Attention <br> (-) Transformer architecture | :material-close: | :material-close: |
|   08    | Generative Models | (-) Introduction to generative models <br> (-) GAN architecture <br> (-) Popular GAN variants | :material-close: | :material-close: |
|   09    | Hyperparameter Tuning | (-) Choosing the best hyperparameters <br> (-) Hyperparameter optimization methods | :material-close: | :material-close: |

## V. Acknowledgements

This course is inspired by the collective knowledge and contributions of the deep learning community. I would like to extend my gratitude to researchers, educators and institutions whose work has shaped modern AI, including [**Stanford CS230 - Deep Learning (Andrew Ng)**](http://cs230.stanford.edu/syllabus/), and key research papers on neural networks, transformers and generative models.

---
<br>